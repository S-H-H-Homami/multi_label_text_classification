{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-H-H-Homami/multi_label_text_classification/blob/main/multi_label_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-label text classification\n",
        "\n",
        "**Author:** [Farrokh Karimi](https://farrokhkarimi.github.io/)  \n",
        "**Description:** In this notebook, we want to classify the Ronash dataset into 20 category."
      ],
      "metadata": {
        "id": "Bij91pNWQOR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "sL739kMAEsU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading Data from the Google Drive link\n",
        "!gdown 1Yq3XTnACkvaIiNlhX09Zth55nOau7jQy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHa9m3aLEbyP",
        "outputId": "6f8136ea-d0d7-42df-a1f7-0faaa45ad4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Yq3XTnACkvaIiNlhX09Zth55nOau7jQy\n",
            "To: /content/Ronash_DS_Assignment.csv\n",
            "\r  0% 0.00/1.05M [00:00<?, ?B/s]\r100% 1.05M/1.05M [00:00<00:00, 110MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QpBuoWrC5QS",
        "outputId": "d4ce58b8-d0a5-43fb-b659-30d176d66ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ronash_DS_Assignment.csv  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the csv file as a dataframe\n",
        "df = pd.read_csv('Ronash_DS_Assignment.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "722bMOmzF4_H",
        "outputId": "6830d193-1846-499e-9c60-42c9d8936b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         product_id                                              title  \\\n",
              "0     3937721221199    Fidele Super Premium Adult Large Breed Dog Food   \n",
              "1     7353058033889                    Foldable Pet Toys Linen Storage   \n",
              "2     6594773549129                                     Bok Dok Diaper   \n",
              "3     4802008318014                              Tastybone Toy Chicken   \n",
              "4     1779705151539                Leather Leash Tab - Short Dog Leash   \n",
              "...             ...                                                ...   \n",
              "5265  4637089464407                              Candylab MOO Milk Van   \n",
              "5266  4996632444987  Truck - Modern Era Vehicles -- Red, White -  S...   \n",
              "5267  5528541003927  Car Sticker Flags Decal American Flag Sticker for   \n",
              "5268  1395163889730          Lazer Helmets Bayamo Pit Bull - Full Face   \n",
              "5269  3535679324240                             Deutz Agrotron Tractor   \n",
              "\n",
              "                 vendor                                               tags  \\\n",
              "0                Fidele  ['Adult', 'Bangalore', 'Chennai', 'Chicken', '...   \n",
              "1             Cap Point                                                 []   \n",
              "2             Pets Home  ['Brand_Pet Arabia', 'Category_Pets Home', 'Ca...   \n",
              "3             TastyBone                                                 []   \n",
              "4            Mighty Paw                 ['Leash', 'Leash Tab', 'Training']   \n",
              "...                 ...                                                ...   \n",
              "5265           Candylab  ['3 Years +', 'candylab', 'Discount Products',...   \n",
              "5266   Woodland Scenics  ['HO Scale', 'ho-scale-items', 'vehicles', 'wo...   \n",
              "5267        Cyan Selene                                          ['Other']   \n",
              "5268  OPEN BOX BARGAINS  ['65061090', 'Antiscratch Pinlock Ready Visor'...   \n",
              "5269               Siku  ['$0 to $25', 'diecast-models', 'gift-finder',...   \n",
              "\n",
              "                    category  \n",
              "0     Animals & Pet Supplies  \n",
              "1     Animals & Pet Supplies  \n",
              "2     Animals & Pet Supplies  \n",
              "3     Animals & Pet Supplies  \n",
              "4     Animals & Pet Supplies  \n",
              "...                      ...  \n",
              "5265        Vehicles & Parts  \n",
              "5266        Vehicles & Parts  \n",
              "5267        Vehicles & Parts  \n",
              "5268        Vehicles & Parts  \n",
              "5269        Vehicles & Parts  \n",
              "\n",
              "[5270 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-45a0fca6-ffaf-44aa-9593-a863b8d9d240\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product_id</th>\n",
              "      <th>title</th>\n",
              "      <th>vendor</th>\n",
              "      <th>tags</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3937721221199</td>\n",
              "      <td>Fidele Super Premium Adult Large Breed Dog Food</td>\n",
              "      <td>Fidele</td>\n",
              "      <td>['Adult', 'Bangalore', 'Chennai', 'Chicken', '...</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7353058033889</td>\n",
              "      <td>Foldable Pet Toys Linen Storage</td>\n",
              "      <td>Cap Point</td>\n",
              "      <td>[]</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6594773549129</td>\n",
              "      <td>Bok Dok Diaper</td>\n",
              "      <td>Pets Home</td>\n",
              "      <td>['Brand_Pet Arabia', 'Category_Pets Home', 'Ca...</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4802008318014</td>\n",
              "      <td>Tastybone Toy Chicken</td>\n",
              "      <td>TastyBone</td>\n",
              "      <td>[]</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1779705151539</td>\n",
              "      <td>Leather Leash Tab - Short Dog Leash</td>\n",
              "      <td>Mighty Paw</td>\n",
              "      <td>['Leash', 'Leash Tab', 'Training']</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5265</th>\n",
              "      <td>4637089464407</td>\n",
              "      <td>Candylab MOO Milk Van</td>\n",
              "      <td>Candylab</td>\n",
              "      <td>['3 Years +', 'candylab', 'Discount Products',...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5266</th>\n",
              "      <td>4996632444987</td>\n",
              "      <td>Truck - Modern Era Vehicles -- Red, White -  S...</td>\n",
              "      <td>Woodland Scenics</td>\n",
              "      <td>['HO Scale', 'ho-scale-items', 'vehicles', 'wo...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5267</th>\n",
              "      <td>5528541003927</td>\n",
              "      <td>Car Sticker Flags Decal American Flag Sticker for</td>\n",
              "      <td>Cyan Selene</td>\n",
              "      <td>['Other']</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5268</th>\n",
              "      <td>1395163889730</td>\n",
              "      <td>Lazer Helmets Bayamo Pit Bull - Full Face</td>\n",
              "      <td>OPEN BOX BARGAINS</td>\n",
              "      <td>['65061090', 'Antiscratch Pinlock Ready Visor'...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5269</th>\n",
              "      <td>3535679324240</td>\n",
              "      <td>Deutz Agrotron Tractor</td>\n",
              "      <td>Siku</td>\n",
              "      <td>['$0 to $25', 'diecast-models', 'gift-finder',...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5270 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45a0fca6-ffaf-44aa-9593-a863b8d9d240')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-45a0fca6-ffaf-44aa-9593-a863b8d9d240 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-45a0fca6-ffaf-44aa-9593-a863b8d9d240');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# counting the number of each label\n",
        "df['category'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E1ZDXyPlr6y",
        "outputId": "761f0e68-44dd-464b-96e1-e7bb21f0d585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Apparel & Accessories        1000\n",
              "Animals & Pet Supplies        500\n",
              "Food, Beverages & Tobacco     400\n",
              "Sporting Goods                400\n",
              "Luggage & Bags                400\n",
              "Home & Garden                 400\n",
              "Health & Beauty               400\n",
              "Media                         300\n",
              "Toys & Games                  300\n",
              "Furniture                     200\n",
              "Baby & Toddler                200\n",
              "Arts & Entertainment          200\n",
              "Electronics                   100\n",
              "Business & Industrial         100\n",
              "Office Supplies               100\n",
              "Vehicles & Parts              100\n",
              "Hardware                       50\n",
              "Cameras & Optics               50\n",
              "Software                       50\n",
              "Religious & Ceremonial         20\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# counting how many indices are duplicated in each column\n",
        "print(f\"There are {sum(df['title'].duplicated())} duplicate title.\")\n",
        "print(f\"There are {sum(df['vendor'].duplicated())} duplicate vondor.\")\n",
        "print(f\"There are {sum(df['tags'].duplicated())} duplicate tags.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un_ggKoLUAEf",
        "outputId": "246f081e-f167-4289-d3e8-48734dad8903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 0 duplicate title.\n",
            "There are 1256 duplicate vondor.\n",
            "There are 716 duplicate tags.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# counting the number of Nan samples\n",
        "df.isnull().values.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD95efZwCRYT",
        "outputId": "ae25dd7d-c39a-4b7c-e861-af8a8a7e23c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are just 3 Nan samples in the dataset so we can ignore them."
      ],
      "metadata": {
        "id": "rhPJQuDEb8oN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the function for extracting and standardizing the sentences\n",
        "def text_extraction(dfi):\n",
        "  # in this function, we concatenate text feature parts of the data as a sentence\n",
        "  sentence = ' '.join([dfi['title'], str(dfi['vendor']), dfi['tags']])\n",
        "  # Remove punctuations\n",
        "  sentence = re.sub('[^a-zA-Z0-9$.]', ' ', sentence)\n",
        "  # Single character removal\n",
        "  sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "  # Removing multiple spaces\n",
        "  sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "  # Changint to lowercase\n",
        "  sentence = sentence.lower()\n",
        "  return sentence\n",
        "\n",
        "# printing 10 sample sentences\n",
        "for i in range(10):\n",
        "  print(text_extraction(df.iloc[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH9UzP7-__P2",
        "outputId": "ac8f39cd-7153-4b57-fb8f-17f4fa7681f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fidele super premium adult large breed dog food fidele adult bangalore chennai chicken doberman dog dry foods fidele german shepherd golden retriever great dane highpriority imported labrador less than 1000 less than 2000 less than 500 mastiff orange pet nutrition \n",
            "foldable pet toys linen storage cap point \n",
            "bok dok diaper pets home brand pet arabia category pets home category small pets supplies type pet home type pet supplies \n",
            "tastybone toy chicken tastybone \n",
            "leather leash tab short dog leash mighty paw leash leash tab training \n",
            "pridebites texas guitar dog toy pride bites brand pridebites toy type plush \n",
            "burns sensitive pork potato burns 10 25 25 50 50 75 adult burns coat dog food food delivery jansale18 natural nonsale19 sensitive size 12kg size 2kg size 6kg skin \n",
            "bully sticks dog toy adog.co bully sticks dog chew toys dog toys \n",
            "kazoo tough giraffe dog toy kazoo brand kazoo june2021 kazoo material plush plush \n",
            "orgo dog biscuits fresh milk petku brand orgo category dogs dogs lifestage all lifestages orgo price rp 0 to rp 100.000 subcategory treats treats \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the dataset\n",
        "dataset = pd.DataFrame(columns=['text', 'label'])\n",
        "for i in range(len(df)):\n",
        "  dataset = dataset.append({'text':text_extraction(df.iloc[i]), 'label':df.iloc[i]['category']}, ignore_index = True)\n",
        "\n",
        "# creating integer labels for multiclass training\n",
        "dataset['label_int'] = pd.Categorical(dataset['label']).codes\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "7x4WG7NC9Xz3",
        "outputId": "c11c05a4-d623-4837-d3b9-6988d8c1d17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text  \\\n",
              "0     fidele super premium adult large breed dog foo...   \n",
              "1            foldable pet toys linen storage cap point    \n",
              "2     bok dok diaper pets home brand pet arabia cate...   \n",
              "3                      tastybone toy chicken tastybone    \n",
              "4     leather leash tab short dog leash mighty paw l...   \n",
              "...                                                 ...   \n",
              "5265  candylab moo milk van candylab 3 years candyla...   \n",
              "5266  truck modern era vehicles red white scale ho w...   \n",
              "5267  car sticker flags decal american flag sticker ...   \n",
              "5268  lazer helmets bayamo pit bull full face open b...   \n",
              "5269  deutz agrotron tractor siku $0 to $25 diecast ...   \n",
              "\n",
              "                       label  label_int  \n",
              "0     Animals & Pet Supplies          0  \n",
              "1     Animals & Pet Supplies          0  \n",
              "2     Animals & Pet Supplies          0  \n",
              "3     Animals & Pet Supplies          0  \n",
              "4     Animals & Pet Supplies          0  \n",
              "...                      ...        ...  \n",
              "5265        Vehicles & Parts         19  \n",
              "5266        Vehicles & Parts         19  \n",
              "5267        Vehicles & Parts         19  \n",
              "5268        Vehicles & Parts         19  \n",
              "5269        Vehicles & Parts         19  \n",
              "\n",
              "[5270 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e249e259-344e-4137-a67f-0f6d8620e2b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>label_int</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fidele super premium adult large breed dog foo...</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>foldable pet toys linen storage cap point</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bok dok diaper pets home brand pet arabia cate...</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tastybone toy chicken tastybone</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>leather leash tab short dog leash mighty paw l...</td>\n",
              "      <td>Animals &amp; Pet Supplies</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5265</th>\n",
              "      <td>candylab moo milk van candylab 3 years candyla...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5266</th>\n",
              "      <td>truck modern era vehicles red white scale ho w...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5267</th>\n",
              "      <td>car sticker flags decal american flag sticker ...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5268</th>\n",
              "      <td>lazer helmets bayamo pit bull full face open b...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5269</th>\n",
              "      <td>deutz agrotron tractor siku $0 to $25 diecast ...</td>\n",
              "      <td>Vehicles &amp; Parts</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5270 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e249e259-344e-4137-a67f-0f6d8620e2b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e249e259-344e-4137-a67f-0f6d8620e2b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e249e259-344e-4137-a67f-0f6d8620e2b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the names of the labels\n",
        "labels_names = list(Counter(dataset['label']).keys())\n",
        "labels_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uHdTY3p9Xfr",
        "outputId": "e299a3f8-1928-46c7-bbfd-d744bc760868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Animals & Pet Supplies',\n",
              " 'Apparel & Accessories',\n",
              " 'Arts & Entertainment',\n",
              " 'Baby & Toddler',\n",
              " 'Business & Industrial',\n",
              " 'Cameras & Optics',\n",
              " 'Electronics',\n",
              " 'Food, Beverages & Tobacco',\n",
              " 'Furniture',\n",
              " 'Hardware',\n",
              " 'Health & Beauty',\n",
              " 'Home & Garden',\n",
              " 'Luggage & Bags',\n",
              " 'Media',\n",
              " 'Office Supplies',\n",
              " 'Religious & Ceremonial',\n",
              " 'Software',\n",
              " 'Sporting Goods',\n",
              " 'Toys & Games',\n",
              " 'Vehicles & Parts']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing each integer label and its corresponding name label\n",
        "for i, label in enumerate(labels_names):\n",
        "  print(\"Label\", i, \"corresponds to\", label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXCgd6QTC_k6",
        "outputId": "9d9e6d7d-986f-4cde-9941-ffb239aaea40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 0 corresponds to Animals & Pet Supplies\n",
            "Label 1 corresponds to Apparel & Accessories\n",
            "Label 2 corresponds to Arts & Entertainment\n",
            "Label 3 corresponds to Baby & Toddler\n",
            "Label 4 corresponds to Business & Industrial\n",
            "Label 5 corresponds to Cameras & Optics\n",
            "Label 6 corresponds to Electronics\n",
            "Label 7 corresponds to Food, Beverages & Tobacco\n",
            "Label 8 corresponds to Furniture\n",
            "Label 9 corresponds to Hardware\n",
            "Label 10 corresponds to Health & Beauty\n",
            "Label 11 corresponds to Home & Garden\n",
            "Label 12 corresponds to Luggage & Bags\n",
            "Label 13 corresponds to Media\n",
            "Label 14 corresponds to Office Supplies\n",
            "Label 15 corresponds to Religious & Ceremonial\n",
            "Label 16 corresponds to Software\n",
            "Label 17 corresponds to Sporting Goods\n",
            "Label 18 corresponds to Toys & Games\n",
            "Label 19 corresponds to Vehicles & Parts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting dataset to train, validation, and test dataframes\n",
        "train_df, test_df= train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "val_df = test_df.sample(frac=0.5)\n",
        "test_df.drop(val_df.index, inplace=True)\n",
        "\n",
        "print(f\"Number of samples in training set: {len(train_df)}\")\n",
        "print(f\"Number of samples in validation set: {len(val_df)}\")\n",
        "print(f\"Number of samples in test set: {len(test_df)}\")\n",
        "\n",
        "# extracting texts and labels from dataframes\n",
        "train_texts = train_df['text']\n",
        "train_labels = train_df['label_int']\n",
        "val_texts = val_df['text']\n",
        "val_labels = val_df['label_int']\n",
        "test_texts = test_df['text']\n",
        "test_labels = test_df['label_int']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWd2Rfp69X8g",
        "outputId": "86d02eba-1cda-48e5-c000-b70715e943fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in training set: 4216\n",
            "Number of samples in validation set: 527\n",
            "Number of samples in test set: 527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating data generators with batch size 32\n",
        "batch_size = 32\n",
        "raw_train_batch = tf.data.Dataset.from_tensor_slices((train_texts, train_labels)).batch(batch_size)\n",
        "raw_val_batch = tf.data.Dataset.from_tensor_slices((val_texts, val_labels)).batch(batch_size)\n",
        "raw_test_batch = tf.data.Dataset.from_tensor_slices((test_texts, test_labels)).batch(batch_size)\n",
        "\n",
        "# printing texts and labels of a batch of raw train\n",
        "for text, label in raw_train_batch.take(1):\n",
        "  print('Texts: {}'.format(text))\n",
        "  print('labels: {}'.format(label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4kA4X60zkC3",
        "outputId": "c5433084-e2f5-4643-858b-cc0e6e2c11fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts: [b'housie tambola game by brands with foldable reusable tickets snooplay 13 17 year olds 18 years above 251 500 8 12 year olds age no bar all time favourite below 1000 below 400 below 500 best selling board games creative games gifts customer favourites diwali family friends night family games friends family nights geek gifts for boys gifts for friends gifts for girls gifts for kids gifts for parents googleshopping nerd or geek new new collection outdoor games party accessories party essentials party freak party games party games for grown ups party games for kids premium unique new toys '\n",
            " b'foldable waterproof raised dog bed dogiti '\n",
            " b'quadrello di bufala cheese cut wrapped by igourmet category cheese cut cheeses milk type buffalo nutrition full set origin italy shipping perishable texture semi soft type stinky and washed rind wholesale cheese collection '\n",
            " b'quay vip pink navy to pink lens quay accessories new summer sunglasses '\n",
            " b'seachem multitest ammonia seachem '\n",
            " b'smart electric cat teasing toy pet clever cat toys cats rr track catproducts rr track cattoys '\n",
            " b'petroleum mixture sign the sign shed black text chemical chemical signs flammable signs general hazard portrait yellow bg '\n",
            " b'scott kingery andrew mccutchen philadelphia phillies ring the bell dual bobblehead foco data athlete andrew mccutchen data athlete scott kingery data city philadelphia data edition exclusive data filter collection no data filter discount yes data league mlb data region pennsylvania data sub category1 bobbleheads data team philadelphia phillies '\n",
            " b'diy personalised dreamcatcher activity kit sea animal doxbox store '\n",
            " b'dainty corduroy bow in lilac sweet first love bow corduroy headband purple spo disabled wos '\n",
            " b'new bts stylish backpack the pocket store '\n",
            " b'planet bike big buck fat front planet bike bicycle fenders bike accessories bikes fenders front fenders full price planet bike unisex '\n",
            " b'ms. better trans canada bitters enterprises jesemi inc. betters canada ms trans '\n",
            " b'drawstring backpack pink zig zag yoobi 10 15 2020 primedaydeal accessories all bags flair back to school backpacks bags bf2019 book bag book bags bookbag bookbags newyear pink pink zig zag sale summer2020 supplybundle zig zags '\n",
            " b'pineapple mini food speaker magnum brands add on birthday gift girl music pos speaker '\n",
            " b'interactive iq treat ball toy for dogs cats cj feeder waterer pet supplies '\n",
            " b'whisper of lord ganesha tarot deck u.s. games '\n",
            " b'handcrafted aasha cuff bracelet colorful cuff bracelet made with wooden beads wrapped with recycled sari fabrics. aasha bracelet comfy fair trade handcrafted handmade jewelry '\n",
            " b'idrop belik facial anti aging wrinkle ultrasonic face massager toothbrush attachment idrop anti aging anti wrinkle electric massager face massage facial massager health health beauty health fitness health care healthcare healthy healthy massager massage massager new ultrasonic '\n",
            " b'wood nativity puzzle mud pie one coas 10760023 baby children christmas gift god jesus kid religion toy '\n",
            " b'p.l.a.y. zoomierex fantastug sea foam dog toy p.l.a.y. brand p.l.a.y. summer collection toy type fetch toy type floating toy type rubber toy type tug twr '\n",
            " b'wallis simpson signed autobiography paul fraser collectibles autographs '\n",
            " b'imperial earring daya jewelry brass flower gold imperial spo default spo disabled spo enabled star tribal '\n",
            " b'armadillo nest charcoal armadillo alfresco armadillo door mats down to earth floor homeware homeware rugs mats labour weekend living modern boho rug rugs mats '\n",
            " b'millie pillow foreside pillows textiles '\n",
            " b'jelly brush aprilskin.us all '\n",
            " b'alarm you re little bitch blue blue blue crew cute exclude feed agegroup adult feed cl0 regularprice womens sassy funny words inappropriate cute profanity swear words exclude rude crude feed color blue feed cond new feed gender feed gender female feed gpc 209 funny inappropriate print profanity red regularprice rude crude sassy socks swear words womens words '\n",
            " b'boutique de paris lashes4today glamorous lashes luxury '\n",
            " b'dog pakiet dnp pc skin support pick me pets brand dolina noteci category pet supplies type dog food '\n",
            " b'plaza weekend diaper bag 7am label final sale addswatchrow adult age adult category diaper bags category weekenders color black color grey diaper diaper diaper ba dipper style plaza bag voyage '\n",
            " b'water sterling silver necklace wear the peace 925 sterling silver all products necklaces new arrivals '\n",
            " b'bain ultra violet rastase adha2020 adjusted assaad back in stock items bahaa black friday 2020 enabled blond absolu blondabsolu ramadan21 btpcat main 132768825422 btpcat other 132768825422 elian dada fadia el mendelek for colored blond hair hair care haircare summerfiesta sale jan 4 2021 price increase july 28th updated july increase ker aprist1st 2021 increase ker august 2021 price increase ker march 1st 2021 price increase ker priceincrease june1st 2021 kerastase ramadan21 rastase loolia2y mahmoud al zarif michel zeytoun non solar october 1st 2020 price increase or or reviews ppd price decrease 16th september 2021 ppd price decrease sept 2021 ppd price increase october1st 2021 ppd pricedecrease midseptember2021 ppd priceincrease 12th october 2021 price updated shampoos shant tawetian shop all stockcount 31 03 2021 weekend summerfiesta sale ']\n",
            "labels: [18  0  7  1  0  0  4  2  0  1 12 17  7 12 18  0 15  1 10 18  0  2  1 11\n",
            "  2  9  1 10  0 12  1 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# counting how many words are there in the whole texts of the dataset\n",
        "num_of_words = 0\n",
        "for i in dataset['text']: num_of_words += len(i.split())\n",
        "\n",
        "print(num_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2QpUriFrsU_",
        "outputId": "0811f810-2bad-4500-bff8-9545307ad7ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are about 112000 words in the texts.\n",
        "\n"
      ],
      "metadata": {
        "id": "wfw3CqfZsWyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# counting max sequence length and how many non-repetitive words are there in the whole texts of the dataset\n",
        "l = []\n",
        "max_seq_lenght = 0\n",
        "for i in dataset['text']:\n",
        "  lenght = len(i.split())\n",
        "  if lenght > max_seq_lenght: max_seq_lenght = lenght\n",
        "  for j in i.split():\n",
        "    if j not in l: l.append(j)\n",
        "\n",
        "print(max_seq_lenght)\n",
        "print(len(l))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ufQzaF9ncqZ",
        "outputId": "9799d557-5234-4969-9e1a-c630b9adac4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "309\n",
            "18933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum sequence length is 309 and There are about 19000 non-repetitive words in the whole dataset texts. So we set max word features to 18933 and sequence length to 309."
      ],
      "metadata": {
        "id": "cGGf8rQorkSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the text vectorization layer with 18933 words and 309 sequence length\n",
        "max_features = len(l)\n",
        "sequence_length = max_seq_lenght\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# fitting the state of the preprocessing layer to the train set. This will cause the model to build an index of strings to integers.\n",
        "vectorize_layer.adapt(train_texts)\n",
        "\n",
        "# defining the vectorize text function\n",
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label\n",
        "\n",
        "# retrieving a sample from a batch of texts and labels from the train set\n",
        "text_batch, label_batch = next(iter(raw_train_batch))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", first_label)\n",
        "print(\"Vectorized text\", vectorize_text(first_review, first_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey2QJjO97T9P",
        "outputId": "dfeeb9fc-4019-41a9-da18-73fec527db9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review tf.Tensor(b'housie tambola game by brands with foldable reusable tickets snooplay 13 17 year olds 18 years above 251 500 8 12 year olds age no bar all time favourite below 1000 below 400 below 500 best selling board games creative games gifts customer favourites diwali family friends night family games friends family nights geek gifts for boys gifts for friends gifts for girls gifts for kids gifts for parents googleshopping nerd or geek new new collection outdoor games party accessories party essentials party freak party games party games for grown ups party games for kids premium unique new toys ', shape=(), dtype=string)\n",
            "Label tf.Tensor(18, shape=(), dtype=int8)\n",
            "Vectorized text (<tf.Tensor: shape=(1, 309), dtype=int64, numpy=\n",
            "array([[12501,  9229,   192,    23,   343,    51,  1546,  2392,  9089,\n",
            "         9635,   740,  1609,   275,  2882,   372,    69,   959, 15878,\n",
            "          134,   109,    89,   275,  2882,    35,    55,   381,    16,\n",
            "          480,  7346,   783,   181,   783,   961,   783,   134,    79,\n",
            "         2792,   104,    57,  1195,    57,    53,  3072, 13287,  5047,\n",
            "          237,  1078,   716,   237,    57,  1078,   237,  6529,  4896,\n",
            "           53,    13,   334,    53,    13,  1078,    53,    13,   179,\n",
            "           53,    13,    62,    53,    13,  3473, 12859, 11174,   303,\n",
            "         4896,     3,     3,     8,   157,    57,   250,    17,   250,\n",
            "          316,   250,  7261,   250,    57,   250,    57,    13,  1904,\n",
            "         5674,   250,    57,    13,    62,   360,   632,     3,     6,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0]])>, <tf.Tensor: shape=(), dtype=int8, numpy=18>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting corresponding word of each integer \n",
        "print(\"1401 ---> \",vectorize_layer.get_vocabulary()[1401])\n",
        "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X84dkB768GSx",
        "outputId": "491602e2-2d65-4e58-ce79-f62b656263a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1401 --->  heel\n",
            " 313 --->  is\n",
            "Vocabulary size: 16230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating train, val, and test vectorized dataset and prefetching them\n",
        "train_ds = raw_train_batch.map(vectorize_text)\n",
        "val_ds = raw_val_batch.map(vectorize_text)\n",
        "test_ds = raw_test_batch.map(vectorize_text)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "DiW717GQ_77D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model configuration\n",
        "## Dropeout percentages are set to 50% and the activation function is set as \"Softmax\"\n",
        "\n",
        "embedding_dim = 32\n",
        "num_of_labels = 20\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.5),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.5),\n",
        "  layers.Dense(num_of_labels, activation= 'softplus')])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBHmGG_SAWhp",
        "outputId": "cb34ccec-6b01-45f6-e1ee-049d92f4155d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          605888    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 32)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 32)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20)                660       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 606,548\n",
            "Trainable params: 606,548\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model compilation\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uL6c-YGfAwLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model\n",
        "epochs = 2000\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                            patience=300,\n",
        "                                            verbose=1)\n",
        "\n",
        "history = model.fit(train_ds,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIfmPA3SvX4d",
        "outputId": "d159d330-5f06-429c-adc4-e9cb727480ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5585: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "132/132 [==============================] - 19s 133ms/step - loss: 2.9031 - accuracy: 0.1606 - val_loss: 2.7658 - val_accuracy: 0.2068\n",
            "Epoch 2/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.7291 - accuracy: 0.1883 - val_loss: 2.6356 - val_accuracy: 0.2068\n",
            "Epoch 3/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.6934 - accuracy: 0.1879 - val_loss: 2.6194 - val_accuracy: 0.2068\n",
            "Epoch 4/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 2.6843 - accuracy: 0.1876 - val_loss: 2.6131 - val_accuracy: 0.2068\n",
            "Epoch 5/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 2.6804 - accuracy: 0.1881 - val_loss: 2.6090 - val_accuracy: 0.2068\n",
            "Epoch 6/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.6747 - accuracy: 0.1886 - val_loss: 2.6062 - val_accuracy: 0.2068\n",
            "Epoch 7/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.6711 - accuracy: 0.1883 - val_loss: 2.6028 - val_accuracy: 0.2068\n",
            "Epoch 8/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.6670 - accuracy: 0.1886 - val_loss: 2.5979 - val_accuracy: 0.2068\n",
            "Epoch 9/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 2.6586 - accuracy: 0.1883 - val_loss: 2.5923 - val_accuracy: 0.2068\n",
            "Epoch 10/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 2.6516 - accuracy: 0.1888 - val_loss: 2.5865 - val_accuracy: 0.2068\n",
            "Epoch 11/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 2.6437 - accuracy: 0.1883 - val_loss: 2.5819 - val_accuracy: 0.2068\n",
            "Epoch 12/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 2.6322 - accuracy: 0.1898 - val_loss: 2.5754 - val_accuracy: 0.2087\n",
            "Epoch 13/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.6308 - accuracy: 0.1895 - val_loss: 2.5693 - val_accuracy: 0.2087\n",
            "Epoch 14/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 2.6215 - accuracy: 0.1907 - val_loss: 2.5609 - val_accuracy: 0.2106\n",
            "Epoch 15/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 2.6105 - accuracy: 0.1926 - val_loss: 2.5539 - val_accuracy: 0.2106\n",
            "Epoch 16/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.5955 - accuracy: 0.1957 - val_loss: 2.5429 - val_accuracy: 0.2125\n",
            "Epoch 17/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 2.5932 - accuracy: 0.1992 - val_loss: 2.5348 - val_accuracy: 0.2144\n",
            "Epoch 18/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.5727 - accuracy: 0.2040 - val_loss: 2.5216 - val_accuracy: 0.2182\n",
            "Epoch 19/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.5641 - accuracy: 0.2094 - val_loss: 2.5093 - val_accuracy: 0.2201\n",
            "Epoch 20/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.5427 - accuracy: 0.2130 - val_loss: 2.4966 - val_accuracy: 0.2277\n",
            "Epoch 21/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.5295 - accuracy: 0.2208 - val_loss: 2.4808 - val_accuracy: 0.2372\n",
            "Epoch 22/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.5116 - accuracy: 0.2246 - val_loss: 2.4664 - val_accuracy: 0.2486\n",
            "Epoch 23/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.4936 - accuracy: 0.2329 - val_loss: 2.4494 - val_accuracy: 0.2524\n",
            "Epoch 24/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.4732 - accuracy: 0.2358 - val_loss: 2.4331 - val_accuracy: 0.2676\n",
            "Epoch 25/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 2.4511 - accuracy: 0.2457 - val_loss: 2.4135 - val_accuracy: 0.2676\n",
            "Epoch 26/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.4263 - accuracy: 0.2543 - val_loss: 2.3918 - val_accuracy: 0.2789\n",
            "Epoch 27/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.4044 - accuracy: 0.2597 - val_loss: 2.3732 - val_accuracy: 0.2846\n",
            "Epoch 28/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.3821 - accuracy: 0.2685 - val_loss: 2.3519 - val_accuracy: 0.2941\n",
            "Epoch 29/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 2.3567 - accuracy: 0.2754 - val_loss: 2.3282 - val_accuracy: 0.2998\n",
            "Epoch 30/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 2.3231 - accuracy: 0.2868 - val_loss: 2.3044 - val_accuracy: 0.3074\n",
            "Epoch 31/2000\n",
            "132/132 [==============================] - 2s 12ms/step - loss: 2.3001 - accuracy: 0.2936 - val_loss: 2.2804 - val_accuracy: 0.3112\n",
            "Epoch 32/2000\n",
            "132/132 [==============================] - 1s 8ms/step - loss: 2.2728 - accuracy: 0.3043 - val_loss: 2.2547 - val_accuracy: 0.3150\n",
            "Epoch 33/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 2.2435 - accuracy: 0.3188 - val_loss: 2.2293 - val_accuracy: 0.3283\n",
            "Epoch 34/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.2090 - accuracy: 0.3366 - val_loss: 2.2029 - val_accuracy: 0.3378\n",
            "Epoch 35/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 2.1835 - accuracy: 0.3394 - val_loss: 2.1770 - val_accuracy: 0.3567\n",
            "Epoch 36/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.1534 - accuracy: 0.3605 - val_loss: 2.1509 - val_accuracy: 0.3795\n",
            "Epoch 37/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.1139 - accuracy: 0.3809 - val_loss: 2.1224 - val_accuracy: 0.3852\n",
            "Epoch 38/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.0908 - accuracy: 0.3947 - val_loss: 2.0960 - val_accuracy: 0.4061\n",
            "Epoch 39/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.0510 - accuracy: 0.4144 - val_loss: 2.0670 - val_accuracy: 0.4194\n",
            "Epoch 40/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 2.0177 - accuracy: 0.4265 - val_loss: 2.0391 - val_accuracy: 0.4269\n",
            "Epoch 41/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 1.9900 - accuracy: 0.4559 - val_loss: 2.0126 - val_accuracy: 0.4497\n",
            "Epoch 42/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.9530 - accuracy: 0.4787 - val_loss: 1.9833 - val_accuracy: 0.4687\n",
            "Epoch 43/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.9173 - accuracy: 0.4893 - val_loss: 1.9546 - val_accuracy: 0.4687\n",
            "Epoch 44/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.8847 - accuracy: 0.5164 - val_loss: 1.9282 - val_accuracy: 0.4953\n",
            "Epoch 45/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.8483 - accuracy: 0.5270 - val_loss: 1.9013 - val_accuracy: 0.5047\n",
            "Epoch 46/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.8156 - accuracy: 0.5510 - val_loss: 1.8749 - val_accuracy: 0.5351\n",
            "Epoch 47/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 1.7870 - accuracy: 0.5667 - val_loss: 1.8467 - val_accuracy: 0.5389\n",
            "Epoch 48/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 1.7542 - accuracy: 0.5908 - val_loss: 1.8212 - val_accuracy: 0.5674\n",
            "Epoch 49/2000\n",
            "132/132 [==============================] - 1s 7ms/step - loss: 1.7176 - accuracy: 0.5925 - val_loss: 1.7944 - val_accuracy: 0.5769\n",
            "Epoch 50/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 1.6903 - accuracy: 0.6134 - val_loss: 1.7688 - val_accuracy: 0.5920\n",
            "Epoch 51/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 1.6524 - accuracy: 0.6385 - val_loss: 1.7439 - val_accuracy: 0.5939\n",
            "Epoch 52/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.6238 - accuracy: 0.6430 - val_loss: 1.7165 - val_accuracy: 0.6224\n",
            "Epoch 53/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.5927 - accuracy: 0.6686 - val_loss: 1.6919 - val_accuracy: 0.6300\n",
            "Epoch 54/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.5599 - accuracy: 0.6679 - val_loss: 1.6702 - val_accuracy: 0.6509\n",
            "Epoch 55/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.5326 - accuracy: 0.6750 - val_loss: 1.6433 - val_accuracy: 0.6433\n",
            "Epoch 56/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.5018 - accuracy: 0.6981 - val_loss: 1.6205 - val_accuracy: 0.6546\n",
            "Epoch 57/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.4654 - accuracy: 0.7040 - val_loss: 1.5993 - val_accuracy: 0.6603\n",
            "Epoch 58/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.4433 - accuracy: 0.7132 - val_loss: 1.5769 - val_accuracy: 0.6755\n",
            "Epoch 59/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.4129 - accuracy: 0.7225 - val_loss: 1.5562 - val_accuracy: 0.6679\n",
            "Epoch 60/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.3905 - accuracy: 0.7308 - val_loss: 1.5302 - val_accuracy: 0.6774\n",
            "Epoch 61/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 1.3537 - accuracy: 0.7415 - val_loss: 1.5126 - val_accuracy: 0.6793\n",
            "Epoch 62/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.3341 - accuracy: 0.7543 - val_loss: 1.4914 - val_accuracy: 0.6774\n",
            "Epoch 63/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.3086 - accuracy: 0.7576 - val_loss: 1.4720 - val_accuracy: 0.7040\n",
            "Epoch 64/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 1.2855 - accuracy: 0.7661 - val_loss: 1.4489 - val_accuracy: 0.6945\n",
            "Epoch 65/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.2698 - accuracy: 0.7531 - val_loss: 1.4324 - val_accuracy: 0.7021\n",
            "Epoch 66/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.2336 - accuracy: 0.7716 - val_loss: 1.4126 - val_accuracy: 0.7059\n",
            "Epoch 67/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.2160 - accuracy: 0.7716 - val_loss: 1.3972 - val_accuracy: 0.7078\n",
            "Epoch 68/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 1.1903 - accuracy: 0.7799 - val_loss: 1.3805 - val_accuracy: 0.7343\n",
            "Epoch 69/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.1742 - accuracy: 0.7865 - val_loss: 1.3620 - val_accuracy: 0.7343\n",
            "Epoch 70/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 1.1480 - accuracy: 0.7908 - val_loss: 1.3421 - val_accuracy: 0.7249\n",
            "Epoch 71/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 1.1234 - accuracy: 0.7946 - val_loss: 1.3309 - val_accuracy: 0.7457\n",
            "Epoch 72/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 1.1080 - accuracy: 0.7951 - val_loss: 1.3127 - val_accuracy: 0.7438\n",
            "Epoch 73/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.0881 - accuracy: 0.8057 - val_loss: 1.2944 - val_accuracy: 0.7495\n",
            "Epoch 74/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.0640 - accuracy: 0.8095 - val_loss: 1.2807 - val_accuracy: 0.7552\n",
            "Epoch 75/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.0416 - accuracy: 0.8072 - val_loss: 1.2669 - val_accuracy: 0.7571\n",
            "Epoch 76/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.0215 - accuracy: 0.8100 - val_loss: 1.2559 - val_accuracy: 0.7685\n",
            "Epoch 77/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 1.0124 - accuracy: 0.8121 - val_loss: 1.2381 - val_accuracy: 0.7571\n",
            "Epoch 78/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.9899 - accuracy: 0.8169 - val_loss: 1.2262 - val_accuracy: 0.7552\n",
            "Epoch 79/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.9829 - accuracy: 0.8226 - val_loss: 1.2157 - val_accuracy: 0.7666\n",
            "Epoch 80/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.9539 - accuracy: 0.8280 - val_loss: 1.2022 - val_accuracy: 0.7704\n",
            "Epoch 81/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.9417 - accuracy: 0.8245 - val_loss: 1.1890 - val_accuracy: 0.7723\n",
            "Epoch 82/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.9289 - accuracy: 0.8264 - val_loss: 1.1760 - val_accuracy: 0.7647\n",
            "Epoch 83/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.9158 - accuracy: 0.8356 - val_loss: 1.1625 - val_accuracy: 0.7666\n",
            "Epoch 84/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.8877 - accuracy: 0.8328 - val_loss: 1.1530 - val_accuracy: 0.7780\n",
            "Epoch 85/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.8809 - accuracy: 0.8366 - val_loss: 1.1420 - val_accuracy: 0.7818\n",
            "Epoch 86/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.8671 - accuracy: 0.8430 - val_loss: 1.1314 - val_accuracy: 0.7761\n",
            "Epoch 87/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.8490 - accuracy: 0.8446 - val_loss: 1.1206 - val_accuracy: 0.7742\n",
            "Epoch 88/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.8450 - accuracy: 0.8444 - val_loss: 1.1113 - val_accuracy: 0.7780\n",
            "Epoch 89/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.8267 - accuracy: 0.8501 - val_loss: 1.1008 - val_accuracy: 0.7780\n",
            "Epoch 90/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.8040 - accuracy: 0.8491 - val_loss: 1.0916 - val_accuracy: 0.7856\n",
            "Epoch 91/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.7989 - accuracy: 0.8489 - val_loss: 1.0816 - val_accuracy: 0.7818\n",
            "Epoch 92/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.7919 - accuracy: 0.8584 - val_loss: 1.0729 - val_accuracy: 0.7799\n",
            "Epoch 93/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.7718 - accuracy: 0.8520 - val_loss: 1.0684 - val_accuracy: 0.7875\n",
            "Epoch 94/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.7607 - accuracy: 0.8629 - val_loss: 1.0589 - val_accuracy: 0.7818\n",
            "Epoch 95/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.7548 - accuracy: 0.8598 - val_loss: 1.0494 - val_accuracy: 0.7875\n",
            "Epoch 96/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.7351 - accuracy: 0.8641 - val_loss: 1.0420 - val_accuracy: 0.7837\n",
            "Epoch 97/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.7298 - accuracy: 0.8636 - val_loss: 1.0340 - val_accuracy: 0.7856\n",
            "Epoch 98/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.7181 - accuracy: 0.8698 - val_loss: 1.0267 - val_accuracy: 0.7989\n",
            "Epoch 99/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.7072 - accuracy: 0.8722 - val_loss: 1.0171 - val_accuracy: 0.7970\n",
            "Epoch 100/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.6985 - accuracy: 0.8712 - val_loss: 1.0097 - val_accuracy: 0.8008\n",
            "Epoch 101/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.6869 - accuracy: 0.8762 - val_loss: 1.0042 - val_accuracy: 0.8008\n",
            "Epoch 102/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.6768 - accuracy: 0.8776 - val_loss: 0.9951 - val_accuracy: 0.7951\n",
            "Epoch 103/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.6615 - accuracy: 0.8759 - val_loss: 0.9889 - val_accuracy: 0.8008\n",
            "Epoch 104/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.6650 - accuracy: 0.8752 - val_loss: 0.9821 - val_accuracy: 0.7951\n",
            "Epoch 105/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.8824 - val_loss: 0.9750 - val_accuracy: 0.8027\n",
            "Epoch 106/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.6405 - accuracy: 0.8843 - val_loss: 0.9681 - val_accuracy: 0.7989\n",
            "Epoch 107/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.6383 - accuracy: 0.8783 - val_loss: 0.9605 - val_accuracy: 0.7989\n",
            "Epoch 108/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.6177 - accuracy: 0.8880 - val_loss: 0.9607 - val_accuracy: 0.8008\n",
            "Epoch 109/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.8902 - val_loss: 0.9522 - val_accuracy: 0.8046\n",
            "Epoch 110/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.6027 - accuracy: 0.8885 - val_loss: 0.9491 - val_accuracy: 0.8046\n",
            "Epoch 111/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.5917 - accuracy: 0.8956 - val_loss: 0.9412 - val_accuracy: 0.8083\n",
            "Epoch 112/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.5907 - accuracy: 0.8916 - val_loss: 0.9336 - val_accuracy: 0.8083\n",
            "Epoch 113/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.5792 - accuracy: 0.8944 - val_loss: 0.9309 - val_accuracy: 0.8102\n",
            "Epoch 114/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.5734 - accuracy: 0.9009 - val_loss: 0.9283 - val_accuracy: 0.8140\n",
            "Epoch 115/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.5711 - accuracy: 0.8926 - val_loss: 0.9187 - val_accuracy: 0.8065\n",
            "Epoch 116/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.5567 - accuracy: 0.9004 - val_loss: 0.9167 - val_accuracy: 0.8140\n",
            "Epoch 117/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.5588 - accuracy: 0.8982 - val_loss: 0.9141 - val_accuracy: 0.8102\n",
            "Epoch 118/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.5371 - accuracy: 0.9018 - val_loss: 0.9053 - val_accuracy: 0.8083\n",
            "Epoch 119/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.5358 - accuracy: 0.9001 - val_loss: 0.9009 - val_accuracy: 0.8083\n",
            "Epoch 120/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.5291 - accuracy: 0.9061 - val_loss: 0.8970 - val_accuracy: 0.8140\n",
            "Epoch 121/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.5266 - accuracy: 0.9037 - val_loss: 0.8910 - val_accuracy: 0.8102\n",
            "Epoch 122/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.5162 - accuracy: 0.9077 - val_loss: 0.8888 - val_accuracy: 0.8159\n",
            "Epoch 123/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.5116 - accuracy: 0.9065 - val_loss: 0.8875 - val_accuracy: 0.8121\n",
            "Epoch 124/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.5027 - accuracy: 0.9134 - val_loss: 0.8820 - val_accuracy: 0.8140\n",
            "Epoch 125/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.4924 - accuracy: 0.9139 - val_loss: 0.8795 - val_accuracy: 0.8140\n",
            "Epoch 126/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.4875 - accuracy: 0.9134 - val_loss: 0.8736 - val_accuracy: 0.8178\n",
            "Epoch 127/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.4858 - accuracy: 0.9096 - val_loss: 0.8721 - val_accuracy: 0.8197\n",
            "Epoch 128/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.4869 - accuracy: 0.9103 - val_loss: 0.8663 - val_accuracy: 0.8216\n",
            "Epoch 129/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.4742 - accuracy: 0.9167 - val_loss: 0.8616 - val_accuracy: 0.8216\n",
            "Epoch 130/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.4687 - accuracy: 0.9144 - val_loss: 0.8598 - val_accuracy: 0.8140\n",
            "Epoch 131/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.4629 - accuracy: 0.9194 - val_loss: 0.8577 - val_accuracy: 0.8216\n",
            "Epoch 132/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.4594 - accuracy: 0.9175 - val_loss: 0.8526 - val_accuracy: 0.8197\n",
            "Epoch 133/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.4560 - accuracy: 0.9167 - val_loss: 0.8526 - val_accuracy: 0.8121\n",
            "Epoch 134/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.4454 - accuracy: 0.9208 - val_loss: 0.8514 - val_accuracy: 0.8235\n",
            "Epoch 135/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.4420 - accuracy: 0.9239 - val_loss: 0.8447 - val_accuracy: 0.8197\n",
            "Epoch 136/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.4304 - accuracy: 0.9255 - val_loss: 0.8428 - val_accuracy: 0.8254\n",
            "Epoch 137/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.4332 - accuracy: 0.9224 - val_loss: 0.8380 - val_accuracy: 0.8216\n",
            "Epoch 138/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.4251 - accuracy: 0.9184 - val_loss: 0.8370 - val_accuracy: 0.8140\n",
            "Epoch 139/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.4250 - accuracy: 0.9215 - val_loss: 0.8335 - val_accuracy: 0.8216\n",
            "Epoch 140/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.4113 - accuracy: 0.9274 - val_loss: 0.8307 - val_accuracy: 0.8254\n",
            "Epoch 141/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.4109 - accuracy: 0.9312 - val_loss: 0.8268 - val_accuracy: 0.8216\n",
            "Epoch 142/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.4077 - accuracy: 0.9262 - val_loss: 0.8224 - val_accuracy: 0.8235\n",
            "Epoch 143/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.3963 - accuracy: 0.9331 - val_loss: 0.8270 - val_accuracy: 0.8159\n",
            "Epoch 144/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.3946 - accuracy: 0.9310 - val_loss: 0.8228 - val_accuracy: 0.8197\n",
            "Epoch 145/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.3922 - accuracy: 0.9310 - val_loss: 0.8199 - val_accuracy: 0.8254\n",
            "Epoch 146/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.3826 - accuracy: 0.9326 - val_loss: 0.8172 - val_accuracy: 0.8235\n",
            "Epoch 147/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.3792 - accuracy: 0.9331 - val_loss: 0.8115 - val_accuracy: 0.8349\n",
            "Epoch 148/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.3774 - accuracy: 0.9293 - val_loss: 0.8109 - val_accuracy: 0.8273\n",
            "Epoch 149/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.3713 - accuracy: 0.9386 - val_loss: 0.8096 - val_accuracy: 0.8235\n",
            "Epoch 150/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.3712 - accuracy: 0.9331 - val_loss: 0.8063 - val_accuracy: 0.8273\n",
            "Epoch 151/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.3634 - accuracy: 0.9329 - val_loss: 0.8063 - val_accuracy: 0.8159\n",
            "Epoch 152/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.3614 - accuracy: 0.9369 - val_loss: 0.8013 - val_accuracy: 0.8311\n",
            "Epoch 153/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.3480 - accuracy: 0.9412 - val_loss: 0.7998 - val_accuracy: 0.8292\n",
            "Epoch 154/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.3515 - accuracy: 0.9383 - val_loss: 0.7970 - val_accuracy: 0.8292\n",
            "Epoch 155/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.3486 - accuracy: 0.9421 - val_loss: 0.7964 - val_accuracy: 0.8254\n",
            "Epoch 156/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.3498 - accuracy: 0.9386 - val_loss: 0.7943 - val_accuracy: 0.8311\n",
            "Epoch 157/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.3416 - accuracy: 0.9457 - val_loss: 0.7916 - val_accuracy: 0.8330\n",
            "Epoch 158/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.3368 - accuracy: 0.9445 - val_loss: 0.7902 - val_accuracy: 0.8273\n",
            "Epoch 159/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.3359 - accuracy: 0.9431 - val_loss: 0.7892 - val_accuracy: 0.8273\n",
            "Epoch 160/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.3309 - accuracy: 0.9452 - val_loss: 0.7878 - val_accuracy: 0.8292\n",
            "Epoch 161/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.3255 - accuracy: 0.9473 - val_loss: 0.7829 - val_accuracy: 0.8254\n",
            "Epoch 162/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.3194 - accuracy: 0.9490 - val_loss: 0.7834 - val_accuracy: 0.8273\n",
            "Epoch 163/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.3144 - accuracy: 0.9466 - val_loss: 0.7838 - val_accuracy: 0.8273\n",
            "Epoch 164/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.9450 - val_loss: 0.7783 - val_accuracy: 0.8368\n",
            "Epoch 165/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.3116 - accuracy: 0.9492 - val_loss: 0.7775 - val_accuracy: 0.8273\n",
            "Epoch 166/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.3065 - accuracy: 0.9485 - val_loss: 0.7772 - val_accuracy: 0.8349\n",
            "Epoch 167/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.3016 - accuracy: 0.9504 - val_loss: 0.7785 - val_accuracy: 0.8254\n",
            "Epoch 168/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.3023 - accuracy: 0.9511 - val_loss: 0.7733 - val_accuracy: 0.8349\n",
            "Epoch 169/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.3020 - accuracy: 0.9497 - val_loss: 0.7687 - val_accuracy: 0.8311\n",
            "Epoch 170/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2950 - accuracy: 0.9511 - val_loss: 0.7709 - val_accuracy: 0.8311\n",
            "Epoch 171/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2902 - accuracy: 0.9519 - val_loss: 0.7681 - val_accuracy: 0.8273\n",
            "Epoch 172/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2860 - accuracy: 0.9523 - val_loss: 0.7661 - val_accuracy: 0.8292\n",
            "Epoch 173/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.2786 - accuracy: 0.9542 - val_loss: 0.7656 - val_accuracy: 0.8292\n",
            "Epoch 174/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.2801 - accuracy: 0.9519 - val_loss: 0.7614 - val_accuracy: 0.8330\n",
            "Epoch 175/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.2715 - accuracy: 0.9573 - val_loss: 0.7630 - val_accuracy: 0.8254\n",
            "Epoch 176/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.2718 - accuracy: 0.9554 - val_loss: 0.7679 - val_accuracy: 0.8292\n",
            "Epoch 177/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2717 - accuracy: 0.9542 - val_loss: 0.7605 - val_accuracy: 0.8292\n",
            "Epoch 178/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2672 - accuracy: 0.9566 - val_loss: 0.7629 - val_accuracy: 0.8292\n",
            "Epoch 179/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2615 - accuracy: 0.9594 - val_loss: 0.7610 - val_accuracy: 0.8311\n",
            "Epoch 180/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2630 - accuracy: 0.9559 - val_loss: 0.7580 - val_accuracy: 0.8273\n",
            "Epoch 181/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2587 - accuracy: 0.9599 - val_loss: 0.7578 - val_accuracy: 0.8330\n",
            "Epoch 182/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2553 - accuracy: 0.9580 - val_loss: 0.7563 - val_accuracy: 0.8330\n",
            "Epoch 183/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2567 - accuracy: 0.9568 - val_loss: 0.7518 - val_accuracy: 0.8292\n",
            "Epoch 184/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2544 - accuracy: 0.9554 - val_loss: 0.7583 - val_accuracy: 0.8311\n",
            "Epoch 185/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2459 - accuracy: 0.9639 - val_loss: 0.7520 - val_accuracy: 0.8292\n",
            "Epoch 186/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2471 - accuracy: 0.9606 - val_loss: 0.7548 - val_accuracy: 0.8311\n",
            "Epoch 187/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2479 - accuracy: 0.9606 - val_loss: 0.7487 - val_accuracy: 0.8235\n",
            "Epoch 188/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2445 - accuracy: 0.9628 - val_loss: 0.7481 - val_accuracy: 0.8292\n",
            "Epoch 189/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2389 - accuracy: 0.9623 - val_loss: 0.7472 - val_accuracy: 0.8311\n",
            "Epoch 190/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2357 - accuracy: 0.9628 - val_loss: 0.7473 - val_accuracy: 0.8254\n",
            "Epoch 191/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2322 - accuracy: 0.9651 - val_loss: 0.7468 - val_accuracy: 0.8330\n",
            "Epoch 192/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2328 - accuracy: 0.9618 - val_loss: 0.7442 - val_accuracy: 0.8292\n",
            "Epoch 193/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2257 - accuracy: 0.9651 - val_loss: 0.7466 - val_accuracy: 0.8292\n",
            "Epoch 194/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2277 - accuracy: 0.9682 - val_loss: 0.7439 - val_accuracy: 0.8368\n",
            "Epoch 195/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.2201 - accuracy: 0.9680 - val_loss: 0.7433 - val_accuracy: 0.8330\n",
            "Epoch 196/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.2187 - accuracy: 0.9649 - val_loss: 0.7435 - val_accuracy: 0.8311\n",
            "Epoch 197/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2209 - accuracy: 0.9666 - val_loss: 0.7395 - val_accuracy: 0.8292\n",
            "Epoch 198/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2180 - accuracy: 0.9654 - val_loss: 0.7402 - val_accuracy: 0.8292\n",
            "Epoch 199/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2175 - accuracy: 0.9699 - val_loss: 0.7400 - val_accuracy: 0.8273\n",
            "Epoch 200/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2058 - accuracy: 0.9696 - val_loss: 0.7433 - val_accuracy: 0.8330\n",
            "Epoch 201/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2055 - accuracy: 0.9689 - val_loss: 0.7371 - val_accuracy: 0.8273\n",
            "Epoch 202/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2100 - accuracy: 0.9670 - val_loss: 0.7386 - val_accuracy: 0.8406\n",
            "Epoch 203/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2011 - accuracy: 0.9730 - val_loss: 0.7343 - val_accuracy: 0.8311\n",
            "Epoch 204/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2060 - accuracy: 0.9673 - val_loss: 0.7397 - val_accuracy: 0.8292\n",
            "Epoch 205/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1950 - accuracy: 0.9715 - val_loss: 0.7357 - val_accuracy: 0.8292\n",
            "Epoch 206/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2026 - accuracy: 0.9715 - val_loss: 0.7371 - val_accuracy: 0.8311\n",
            "Epoch 207/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1987 - accuracy: 0.9687 - val_loss: 0.7325 - val_accuracy: 0.8311\n",
            "Epoch 208/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1894 - accuracy: 0.9737 - val_loss: 0.7349 - val_accuracy: 0.8273\n",
            "Epoch 209/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1919 - accuracy: 0.9708 - val_loss: 0.7351 - val_accuracy: 0.8254\n",
            "Epoch 210/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1914 - accuracy: 0.9720 - val_loss: 0.7302 - val_accuracy: 0.8368\n",
            "Epoch 211/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1894 - accuracy: 0.9746 - val_loss: 0.7285 - val_accuracy: 0.8349\n",
            "Epoch 212/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1829 - accuracy: 0.9730 - val_loss: 0.7338 - val_accuracy: 0.8311\n",
            "Epoch 213/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1823 - accuracy: 0.9756 - val_loss: 0.7324 - val_accuracy: 0.8273\n",
            "Epoch 214/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1840 - accuracy: 0.9722 - val_loss: 0.7341 - val_accuracy: 0.8273\n",
            "Epoch 215/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.1859 - accuracy: 0.9730 - val_loss: 0.7262 - val_accuracy: 0.8387\n",
            "Epoch 216/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.1823 - accuracy: 0.9744 - val_loss: 0.7256 - val_accuracy: 0.8292\n",
            "Epoch 217/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1794 - accuracy: 0.9727 - val_loss: 0.7286 - val_accuracy: 0.8292\n",
            "Epoch 218/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1704 - accuracy: 0.9791 - val_loss: 0.7256 - val_accuracy: 0.8330\n",
            "Epoch 219/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1742 - accuracy: 0.9770 - val_loss: 0.7272 - val_accuracy: 0.8349\n",
            "Epoch 220/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1781 - accuracy: 0.9722 - val_loss: 0.7265 - val_accuracy: 0.8311\n",
            "Epoch 221/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1659 - accuracy: 0.9789 - val_loss: 0.7260 - val_accuracy: 0.8254\n",
            "Epoch 222/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1695 - accuracy: 0.9758 - val_loss: 0.7268 - val_accuracy: 0.8330\n",
            "Epoch 223/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1705 - accuracy: 0.9775 - val_loss: 0.7292 - val_accuracy: 0.8330\n",
            "Epoch 224/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1609 - accuracy: 0.9803 - val_loss: 0.7281 - val_accuracy: 0.8330\n",
            "Epoch 225/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1628 - accuracy: 0.9798 - val_loss: 0.7277 - val_accuracy: 0.8311\n",
            "Epoch 226/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1588 - accuracy: 0.9806 - val_loss: 0.7229 - val_accuracy: 0.8254\n",
            "Epoch 227/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1640 - accuracy: 0.9791 - val_loss: 0.7238 - val_accuracy: 0.8330\n",
            "Epoch 228/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1602 - accuracy: 0.9784 - val_loss: 0.7249 - val_accuracy: 0.8330\n",
            "Epoch 229/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1592 - accuracy: 0.9791 - val_loss: 0.7222 - val_accuracy: 0.8330\n",
            "Epoch 230/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1533 - accuracy: 0.9803 - val_loss: 0.7213 - val_accuracy: 0.8349\n",
            "Epoch 231/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1535 - accuracy: 0.9824 - val_loss: 0.7225 - val_accuracy: 0.8330\n",
            "Epoch 232/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1537 - accuracy: 0.9803 - val_loss: 0.7219 - val_accuracy: 0.8387\n",
            "Epoch 233/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1455 - accuracy: 0.9815 - val_loss: 0.7198 - val_accuracy: 0.8349\n",
            "Epoch 234/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1520 - accuracy: 0.9791 - val_loss: 0.7220 - val_accuracy: 0.8330\n",
            "Epoch 235/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.1471 - accuracy: 0.9803 - val_loss: 0.7256 - val_accuracy: 0.8349\n",
            "Epoch 236/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.1439 - accuracy: 0.9827 - val_loss: 0.7239 - val_accuracy: 0.8292\n",
            "Epoch 237/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1488 - accuracy: 0.9813 - val_loss: 0.7219 - val_accuracy: 0.8349\n",
            "Epoch 238/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1454 - accuracy: 0.9782 - val_loss: 0.7173 - val_accuracy: 0.8292\n",
            "Epoch 239/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1404 - accuracy: 0.9815 - val_loss: 0.7178 - val_accuracy: 0.8254\n",
            "Epoch 240/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1399 - accuracy: 0.9839 - val_loss: 0.7177 - val_accuracy: 0.8273\n",
            "Epoch 241/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1403 - accuracy: 0.9820 - val_loss: 0.7199 - val_accuracy: 0.8349\n",
            "Epoch 242/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1399 - accuracy: 0.9827 - val_loss: 0.7190 - val_accuracy: 0.8387\n",
            "Epoch 243/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1317 - accuracy: 0.9836 - val_loss: 0.7207 - val_accuracy: 0.8311\n",
            "Epoch 244/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.9824 - val_loss: 0.7208 - val_accuracy: 0.8330\n",
            "Epoch 245/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1295 - accuracy: 0.9881 - val_loss: 0.7193 - val_accuracy: 0.8292\n",
            "Epoch 246/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1282 - accuracy: 0.9834 - val_loss: 0.7237 - val_accuracy: 0.8368\n",
            "Epoch 247/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1346 - accuracy: 0.9843 - val_loss: 0.7175 - val_accuracy: 0.8254\n",
            "Epoch 248/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1290 - accuracy: 0.9843 - val_loss: 0.7208 - val_accuracy: 0.8330\n",
            "Epoch 249/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1289 - accuracy: 0.9829 - val_loss: 0.7196 - val_accuracy: 0.8349\n",
            "Epoch 250/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1264 - accuracy: 0.9846 - val_loss: 0.7195 - val_accuracy: 0.8311\n",
            "Epoch 251/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1239 - accuracy: 0.9841 - val_loss: 0.7165 - val_accuracy: 0.8273\n",
            "Epoch 252/2000\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1274 - accuracy: 0.9841 - val_loss: 0.7189 - val_accuracy: 0.8292\n",
            "Epoch 253/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1247 - accuracy: 0.9834 - val_loss: 0.7140 - val_accuracy: 0.8273\n",
            "Epoch 254/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1231 - accuracy: 0.9848 - val_loss: 0.7171 - val_accuracy: 0.8387\n",
            "Epoch 255/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.1254 - accuracy: 0.9843 - val_loss: 0.7154 - val_accuracy: 0.8254\n",
            "Epoch 256/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.1202 - accuracy: 0.9848 - val_loss: 0.7185 - val_accuracy: 0.8330\n",
            "Epoch 257/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1160 - accuracy: 0.9867 - val_loss: 0.7128 - val_accuracy: 0.8311\n",
            "Epoch 258/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1231 - accuracy: 0.9848 - val_loss: 0.7157 - val_accuracy: 0.8292\n",
            "Epoch 259/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1207 - accuracy: 0.9855 - val_loss: 0.7142 - val_accuracy: 0.8368\n",
            "Epoch 260/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1172 - accuracy: 0.9858 - val_loss: 0.7156 - val_accuracy: 0.8311\n",
            "Epoch 261/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1094 - accuracy: 0.9874 - val_loss: 0.7193 - val_accuracy: 0.8311\n",
            "Epoch 262/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1162 - accuracy: 0.9853 - val_loss: 0.7157 - val_accuracy: 0.8349\n",
            "Epoch 263/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1063 - accuracy: 0.9891 - val_loss: 0.7164 - val_accuracy: 0.8330\n",
            "Epoch 264/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1147 - accuracy: 0.9846 - val_loss: 0.7200 - val_accuracy: 0.8330\n",
            "Epoch 265/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1080 - accuracy: 0.9891 - val_loss: 0.7137 - val_accuracy: 0.8273\n",
            "Epoch 266/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1103 - accuracy: 0.9848 - val_loss: 0.7187 - val_accuracy: 0.8311\n",
            "Epoch 267/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1091 - accuracy: 0.9881 - val_loss: 0.7231 - val_accuracy: 0.8292\n",
            "Epoch 268/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1048 - accuracy: 0.9858 - val_loss: 0.7132 - val_accuracy: 0.8311\n",
            "Epoch 269/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1088 - accuracy: 0.9870 - val_loss: 0.7162 - val_accuracy: 0.8349\n",
            "Epoch 270/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1040 - accuracy: 0.9881 - val_loss: 0.7151 - val_accuracy: 0.8349\n",
            "Epoch 271/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1071 - accuracy: 0.9865 - val_loss: 0.7162 - val_accuracy: 0.8311\n",
            "Epoch 272/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1050 - accuracy: 0.9860 - val_loss: 0.7162 - val_accuracy: 0.8330\n",
            "Epoch 273/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1065 - accuracy: 0.9881 - val_loss: 0.7144 - val_accuracy: 0.8349\n",
            "Epoch 274/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1003 - accuracy: 0.9891 - val_loss: 0.7147 - val_accuracy: 0.8330\n",
            "Epoch 275/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0988 - accuracy: 0.9877 - val_loss: 0.7138 - val_accuracy: 0.8387\n",
            "Epoch 276/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.1008 - accuracy: 0.9886 - val_loss: 0.7123 - val_accuracy: 0.8330\n",
            "Epoch 277/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1006 - accuracy: 0.9862 - val_loss: 0.7146 - val_accuracy: 0.8349\n",
            "Epoch 278/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0944 - accuracy: 0.9900 - val_loss: 0.7171 - val_accuracy: 0.8273\n",
            "Epoch 279/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0995 - accuracy: 0.9870 - val_loss: 0.7139 - val_accuracy: 0.8406\n",
            "Epoch 280/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0959 - accuracy: 0.9910 - val_loss: 0.7212 - val_accuracy: 0.8330\n",
            "Epoch 281/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0986 - accuracy: 0.9886 - val_loss: 0.7144 - val_accuracy: 0.8349\n",
            "Epoch 282/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0938 - accuracy: 0.9903 - val_loss: 0.7191 - val_accuracy: 0.8330\n",
            "Epoch 283/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0907 - accuracy: 0.9898 - val_loss: 0.7166 - val_accuracy: 0.8292\n",
            "Epoch 284/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0944 - accuracy: 0.9903 - val_loss: 0.7117 - val_accuracy: 0.8444\n",
            "Epoch 285/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0971 - accuracy: 0.9870 - val_loss: 0.7122 - val_accuracy: 0.8349\n",
            "Epoch 286/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0895 - accuracy: 0.9886 - val_loss: 0.7118 - val_accuracy: 0.8349\n",
            "Epoch 287/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0944 - accuracy: 0.9870 - val_loss: 0.7168 - val_accuracy: 0.8349\n",
            "Epoch 288/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0858 - accuracy: 0.9907 - val_loss: 0.7161 - val_accuracy: 0.8330\n",
            "Epoch 289/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0899 - accuracy: 0.9910 - val_loss: 0.7186 - val_accuracy: 0.8330\n",
            "Epoch 290/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0847 - accuracy: 0.9891 - val_loss: 0.7104 - val_accuracy: 0.8387\n",
            "Epoch 291/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0874 - accuracy: 0.9915 - val_loss: 0.7118 - val_accuracy: 0.8311\n",
            "Epoch 292/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0875 - accuracy: 0.9893 - val_loss: 0.7149 - val_accuracy: 0.8349\n",
            "Epoch 293/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0865 - accuracy: 0.9889 - val_loss: 0.7137 - val_accuracy: 0.8368\n",
            "Epoch 294/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0836 - accuracy: 0.9924 - val_loss: 0.7132 - val_accuracy: 0.8254\n",
            "Epoch 295/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0856 - accuracy: 0.9896 - val_loss: 0.7144 - val_accuracy: 0.8330\n",
            "Epoch 296/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0803 - accuracy: 0.9903 - val_loss: 0.7166 - val_accuracy: 0.8311\n",
            "Epoch 297/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0776 - accuracy: 0.9931 - val_loss: 0.7132 - val_accuracy: 0.8330\n",
            "Epoch 298/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0826 - accuracy: 0.9879 - val_loss: 0.7129 - val_accuracy: 0.8349\n",
            "Epoch 299/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0811 - accuracy: 0.9907 - val_loss: 0.7160 - val_accuracy: 0.8349\n",
            "Epoch 300/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0773 - accuracy: 0.9931 - val_loss: 0.7143 - val_accuracy: 0.8387\n",
            "Epoch 301/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0831 - accuracy: 0.9922 - val_loss: 0.7155 - val_accuracy: 0.8368\n",
            "Epoch 302/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0815 - accuracy: 0.9926 - val_loss: 0.7189 - val_accuracy: 0.8330\n",
            "Epoch 303/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0740 - accuracy: 0.9924 - val_loss: 0.7171 - val_accuracy: 0.8349\n",
            "Epoch 304/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0788 - accuracy: 0.9922 - val_loss: 0.7185 - val_accuracy: 0.8330\n",
            "Epoch 305/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0752 - accuracy: 0.9917 - val_loss: 0.7188 - val_accuracy: 0.8349\n",
            "Epoch 306/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0747 - accuracy: 0.9919 - val_loss: 0.7155 - val_accuracy: 0.8349\n",
            "Epoch 307/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0761 - accuracy: 0.9912 - val_loss: 0.7163 - val_accuracy: 0.8368\n",
            "Epoch 308/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0759 - accuracy: 0.9912 - val_loss: 0.7166 - val_accuracy: 0.8311\n",
            "Epoch 309/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0734 - accuracy: 0.9929 - val_loss: 0.7188 - val_accuracy: 0.8330\n",
            "Epoch 310/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0719 - accuracy: 0.9926 - val_loss: 0.7231 - val_accuracy: 0.8311\n",
            "Epoch 311/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0699 - accuracy: 0.9922 - val_loss: 0.7238 - val_accuracy: 0.8330\n",
            "Epoch 312/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0741 - accuracy: 0.9905 - val_loss: 0.7103 - val_accuracy: 0.8406\n",
            "Epoch 313/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0711 - accuracy: 0.9931 - val_loss: 0.7153 - val_accuracy: 0.8292\n",
            "Epoch 314/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0701 - accuracy: 0.9922 - val_loss: 0.7189 - val_accuracy: 0.8311\n",
            "Epoch 315/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0689 - accuracy: 0.9936 - val_loss: 0.7179 - val_accuracy: 0.8311\n",
            "Epoch 316/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0707 - accuracy: 0.9917 - val_loss: 0.7223 - val_accuracy: 0.8292\n",
            "Epoch 317/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0717 - accuracy: 0.9917 - val_loss: 0.7136 - val_accuracy: 0.8330\n",
            "Epoch 318/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0699 - accuracy: 0.9943 - val_loss: 0.7195 - val_accuracy: 0.8311\n",
            "Epoch 319/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0687 - accuracy: 0.9936 - val_loss: 0.7215 - val_accuracy: 0.8311\n",
            "Epoch 320/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0655 - accuracy: 0.9931 - val_loss: 0.7224 - val_accuracy: 0.8292\n",
            "Epoch 321/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0672 - accuracy: 0.9917 - val_loss: 0.7177 - val_accuracy: 0.8311\n",
            "Epoch 322/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0647 - accuracy: 0.9943 - val_loss: 0.7261 - val_accuracy: 0.8330\n",
            "Epoch 323/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0634 - accuracy: 0.9938 - val_loss: 0.7173 - val_accuracy: 0.8368\n",
            "Epoch 324/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0640 - accuracy: 0.9929 - val_loss: 0.7109 - val_accuracy: 0.8387\n",
            "Epoch 325/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0653 - accuracy: 0.9929 - val_loss: 0.7205 - val_accuracy: 0.8330\n",
            "Epoch 326/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0648 - accuracy: 0.9945 - val_loss: 0.7159 - val_accuracy: 0.8425\n",
            "Epoch 327/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0653 - accuracy: 0.9926 - val_loss: 0.7237 - val_accuracy: 0.8292\n",
            "Epoch 328/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0616 - accuracy: 0.9948 - val_loss: 0.7238 - val_accuracy: 0.8368\n",
            "Epoch 329/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0650 - accuracy: 0.9926 - val_loss: 0.7208 - val_accuracy: 0.8349\n",
            "Epoch 330/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0606 - accuracy: 0.9945 - val_loss: 0.7197 - val_accuracy: 0.8292\n",
            "Epoch 331/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0618 - accuracy: 0.9938 - val_loss: 0.7191 - val_accuracy: 0.8311\n",
            "Epoch 332/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0614 - accuracy: 0.9945 - val_loss: 0.7184 - val_accuracy: 0.8349\n",
            "Epoch 333/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0630 - accuracy: 0.9938 - val_loss: 0.7215 - val_accuracy: 0.8311\n",
            "Epoch 334/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0600 - accuracy: 0.9936 - val_loss: 0.7195 - val_accuracy: 0.8330\n",
            "Epoch 335/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0560 - accuracy: 0.9938 - val_loss: 0.7141 - val_accuracy: 0.8387\n",
            "Epoch 336/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0570 - accuracy: 0.9945 - val_loss: 0.7248 - val_accuracy: 0.8349\n",
            "Epoch 337/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0644 - accuracy: 0.9934 - val_loss: 0.7219 - val_accuracy: 0.8349\n",
            "Epoch 338/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0589 - accuracy: 0.9931 - val_loss: 0.7254 - val_accuracy: 0.8349\n",
            "Epoch 339/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0574 - accuracy: 0.9953 - val_loss: 0.7177 - val_accuracy: 0.8387\n",
            "Epoch 340/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0602 - accuracy: 0.9929 - val_loss: 0.7192 - val_accuracy: 0.8349\n",
            "Epoch 341/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0551 - accuracy: 0.9945 - val_loss: 0.7192 - val_accuracy: 0.8368\n",
            "Epoch 342/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0619 - accuracy: 0.9924 - val_loss: 0.7209 - val_accuracy: 0.8330\n",
            "Epoch 343/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0557 - accuracy: 0.9938 - val_loss: 0.7250 - val_accuracy: 0.8349\n",
            "Epoch 344/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0552 - accuracy: 0.9934 - val_loss: 0.7208 - val_accuracy: 0.8330\n",
            "Epoch 345/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0545 - accuracy: 0.9929 - val_loss: 0.7195 - val_accuracy: 0.8368\n",
            "Epoch 346/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0559 - accuracy: 0.9941 - val_loss: 0.7211 - val_accuracy: 0.8387\n",
            "Epoch 347/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0561 - accuracy: 0.9936 - val_loss: 0.7209 - val_accuracy: 0.8330\n",
            "Epoch 348/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0534 - accuracy: 0.9941 - val_loss: 0.7237 - val_accuracy: 0.8368\n",
            "Epoch 349/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0597 - accuracy: 0.9922 - val_loss: 0.7261 - val_accuracy: 0.8368\n",
            "Epoch 350/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0533 - accuracy: 0.9934 - val_loss: 0.7296 - val_accuracy: 0.8330\n",
            "Epoch 351/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0510 - accuracy: 0.9955 - val_loss: 0.7260 - val_accuracy: 0.8349\n",
            "Epoch 352/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0524 - accuracy: 0.9936 - val_loss: 0.7273 - val_accuracy: 0.8368\n",
            "Epoch 353/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0556 - accuracy: 0.9945 - val_loss: 0.7270 - val_accuracy: 0.8387\n",
            "Epoch 354/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0548 - accuracy: 0.9938 - val_loss: 0.7258 - val_accuracy: 0.8330\n",
            "Epoch 355/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0498 - accuracy: 0.9941 - val_loss: 0.7232 - val_accuracy: 0.8387\n",
            "Epoch 356/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0503 - accuracy: 0.9967 - val_loss: 0.7187 - val_accuracy: 0.8330\n",
            "Epoch 357/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0517 - accuracy: 0.9953 - val_loss: 0.7277 - val_accuracy: 0.8368\n",
            "Epoch 358/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0521 - accuracy: 0.9957 - val_loss: 0.7243 - val_accuracy: 0.8368\n",
            "Epoch 359/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0485 - accuracy: 0.9950 - val_loss: 0.7200 - val_accuracy: 0.8387\n",
            "Epoch 360/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0475 - accuracy: 0.9943 - val_loss: 0.7274 - val_accuracy: 0.8349\n",
            "Epoch 361/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0475 - accuracy: 0.9953 - val_loss: 0.7312 - val_accuracy: 0.8349\n",
            "Epoch 362/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0502 - accuracy: 0.9955 - val_loss: 0.7188 - val_accuracy: 0.8387\n",
            "Epoch 363/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0504 - accuracy: 0.9953 - val_loss: 0.7392 - val_accuracy: 0.8254\n",
            "Epoch 364/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0495 - accuracy: 0.9957 - val_loss: 0.7286 - val_accuracy: 0.8273\n",
            "Epoch 365/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0495 - accuracy: 0.9950 - val_loss: 0.7254 - val_accuracy: 0.8368\n",
            "Epoch 366/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0489 - accuracy: 0.9948 - val_loss: 0.7258 - val_accuracy: 0.8406\n",
            "Epoch 367/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0473 - accuracy: 0.9953 - val_loss: 0.7272 - val_accuracy: 0.8349\n",
            "Epoch 368/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0451 - accuracy: 0.9962 - val_loss: 0.7234 - val_accuracy: 0.8406\n",
            "Epoch 369/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0479 - accuracy: 0.9943 - val_loss: 0.7258 - val_accuracy: 0.8387\n",
            "Epoch 370/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0474 - accuracy: 0.9948 - val_loss: 0.7287 - val_accuracy: 0.8406\n",
            "Epoch 371/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0436 - accuracy: 0.9962 - val_loss: 0.7289 - val_accuracy: 0.8368\n",
            "Epoch 372/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0453 - accuracy: 0.9967 - val_loss: 0.7262 - val_accuracy: 0.8349\n",
            "Epoch 373/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0461 - accuracy: 0.9945 - val_loss: 0.7351 - val_accuracy: 0.8330\n",
            "Epoch 374/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0441 - accuracy: 0.9972 - val_loss: 0.7350 - val_accuracy: 0.8349\n",
            "Epoch 375/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0420 - accuracy: 0.9964 - val_loss: 0.7386 - val_accuracy: 0.8292\n",
            "Epoch 376/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0459 - accuracy: 0.9948 - val_loss: 0.7281 - val_accuracy: 0.8349\n",
            "Epoch 377/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0461 - accuracy: 0.9929 - val_loss: 0.7278 - val_accuracy: 0.8330\n",
            "Epoch 378/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0422 - accuracy: 0.9967 - val_loss: 0.7298 - val_accuracy: 0.8387\n",
            "Epoch 379/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0415 - accuracy: 0.9967 - val_loss: 0.7380 - val_accuracy: 0.8387\n",
            "Epoch 380/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0400 - accuracy: 0.9955 - val_loss: 0.7310 - val_accuracy: 0.8368\n",
            "Epoch 381/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0440 - accuracy: 0.9960 - val_loss: 0.7354 - val_accuracy: 0.8311\n",
            "Epoch 382/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0454 - accuracy: 0.9948 - val_loss: 0.7307 - val_accuracy: 0.8387\n",
            "Epoch 383/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0416 - accuracy: 0.9957 - val_loss: 0.7360 - val_accuracy: 0.8368\n",
            "Epoch 384/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0420 - accuracy: 0.9960 - val_loss: 0.7390 - val_accuracy: 0.8368\n",
            "Epoch 385/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0434 - accuracy: 0.9948 - val_loss: 0.7408 - val_accuracy: 0.8292\n",
            "Epoch 386/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0420 - accuracy: 0.9962 - val_loss: 0.7378 - val_accuracy: 0.8292\n",
            "Epoch 387/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0402 - accuracy: 0.9964 - val_loss: 0.7272 - val_accuracy: 0.8387\n",
            "Epoch 388/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0429 - accuracy: 0.9945 - val_loss: 0.7348 - val_accuracy: 0.8349\n",
            "Epoch 389/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0402 - accuracy: 0.9964 - val_loss: 0.7369 - val_accuracy: 0.8330\n",
            "Epoch 390/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0419 - accuracy: 0.9941 - val_loss: 0.7337 - val_accuracy: 0.8406\n",
            "Epoch 391/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0379 - accuracy: 0.9960 - val_loss: 0.7319 - val_accuracy: 0.8349\n",
            "Epoch 392/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0398 - accuracy: 0.9950 - val_loss: 0.7384 - val_accuracy: 0.8292\n",
            "Epoch 393/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0411 - accuracy: 0.9957 - val_loss: 0.7270 - val_accuracy: 0.8368\n",
            "Epoch 394/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0371 - accuracy: 0.9969 - val_loss: 0.7320 - val_accuracy: 0.8368\n",
            "Epoch 395/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0407 - accuracy: 0.9950 - val_loss: 0.7334 - val_accuracy: 0.8349\n",
            "Epoch 396/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0381 - accuracy: 0.9964 - val_loss: 0.7304 - val_accuracy: 0.8349\n",
            "Epoch 397/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0376 - accuracy: 0.9953 - val_loss: 0.7336 - val_accuracy: 0.8406\n",
            "Epoch 398/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0384 - accuracy: 0.9974 - val_loss: 0.7345 - val_accuracy: 0.8387\n",
            "Epoch 399/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0397 - accuracy: 0.9962 - val_loss: 0.7334 - val_accuracy: 0.8349\n",
            "Epoch 400/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0411 - accuracy: 0.9953 - val_loss: 0.7382 - val_accuracy: 0.8368\n",
            "Epoch 401/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0398 - accuracy: 0.9948 - val_loss: 0.7291 - val_accuracy: 0.8387\n",
            "Epoch 402/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0369 - accuracy: 0.9967 - val_loss: 0.7313 - val_accuracy: 0.8368\n",
            "Epoch 403/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0364 - accuracy: 0.9960 - val_loss: 0.7382 - val_accuracy: 0.8330\n",
            "Epoch 404/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0354 - accuracy: 0.9976 - val_loss: 0.7362 - val_accuracy: 0.8311\n",
            "Epoch 405/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0315 - accuracy: 0.9967 - val_loss: 0.7383 - val_accuracy: 0.8330\n",
            "Epoch 406/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0329 - accuracy: 0.9957 - val_loss: 0.7421 - val_accuracy: 0.8330\n",
            "Epoch 407/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0379 - accuracy: 0.9967 - val_loss: 0.7430 - val_accuracy: 0.8311\n",
            "Epoch 408/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0341 - accuracy: 0.9967 - val_loss: 0.7395 - val_accuracy: 0.8311\n",
            "Epoch 409/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0359 - accuracy: 0.9955 - val_loss: 0.7400 - val_accuracy: 0.8330\n",
            "Epoch 410/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0353 - accuracy: 0.9976 - val_loss: 0.7371 - val_accuracy: 0.8311\n",
            "Epoch 411/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0355 - accuracy: 0.9969 - val_loss: 0.7416 - val_accuracy: 0.8368\n",
            "Epoch 412/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0339 - accuracy: 0.9974 - val_loss: 0.7388 - val_accuracy: 0.8349\n",
            "Epoch 413/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0365 - accuracy: 0.9950 - val_loss: 0.7354 - val_accuracy: 0.8387\n",
            "Epoch 414/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0389 - accuracy: 0.9948 - val_loss: 0.7331 - val_accuracy: 0.8387\n",
            "Epoch 415/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0340 - accuracy: 0.9964 - val_loss: 0.7405 - val_accuracy: 0.8406\n",
            "Epoch 416/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0373 - accuracy: 0.9957 - val_loss: 0.7403 - val_accuracy: 0.8349\n",
            "Epoch 417/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0310 - accuracy: 0.9979 - val_loss: 0.7327 - val_accuracy: 0.8406\n",
            "Epoch 418/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0353 - accuracy: 0.9964 - val_loss: 0.7431 - val_accuracy: 0.8349\n",
            "Epoch 419/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0346 - accuracy: 0.9950 - val_loss: 0.7389 - val_accuracy: 0.8368\n",
            "Epoch 420/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0349 - accuracy: 0.9945 - val_loss: 0.7388 - val_accuracy: 0.8368\n",
            "Epoch 421/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0364 - accuracy: 0.9953 - val_loss: 0.7364 - val_accuracy: 0.8387\n",
            "Epoch 422/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0322 - accuracy: 0.9976 - val_loss: 0.7387 - val_accuracy: 0.8387\n",
            "Epoch 423/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0335 - accuracy: 0.9964 - val_loss: 0.7442 - val_accuracy: 0.8387\n",
            "Epoch 424/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0318 - accuracy: 0.9972 - val_loss: 0.7385 - val_accuracy: 0.8368\n",
            "Epoch 425/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0321 - accuracy: 0.9976 - val_loss: 0.7490 - val_accuracy: 0.8368\n",
            "Epoch 426/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0329 - accuracy: 0.9962 - val_loss: 0.7393 - val_accuracy: 0.8387\n",
            "Epoch 427/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0336 - accuracy: 0.9964 - val_loss: 0.7492 - val_accuracy: 0.8349\n",
            "Epoch 428/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0322 - accuracy: 0.9967 - val_loss: 0.7426 - val_accuracy: 0.8387\n",
            "Epoch 429/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0297 - accuracy: 0.9974 - val_loss: 0.7427 - val_accuracy: 0.8330\n",
            "Epoch 430/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0305 - accuracy: 0.9969 - val_loss: 0.7476 - val_accuracy: 0.8311\n",
            "Epoch 431/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0319 - accuracy: 0.9957 - val_loss: 0.7470 - val_accuracy: 0.8349\n",
            "Epoch 432/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0322 - accuracy: 0.9955 - val_loss: 0.7479 - val_accuracy: 0.8368\n",
            "Epoch 433/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0311 - accuracy: 0.9964 - val_loss: 0.7521 - val_accuracy: 0.8387\n",
            "Epoch 434/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0325 - accuracy: 0.9960 - val_loss: 0.7549 - val_accuracy: 0.8311\n",
            "Epoch 435/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0299 - accuracy: 0.9983 - val_loss: 0.7526 - val_accuracy: 0.8368\n",
            "Epoch 436/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0290 - accuracy: 0.9960 - val_loss: 0.7550 - val_accuracy: 0.8349\n",
            "Epoch 437/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0337 - accuracy: 0.9962 - val_loss: 0.7469 - val_accuracy: 0.8368\n",
            "Epoch 438/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0300 - accuracy: 0.9964 - val_loss: 0.7401 - val_accuracy: 0.8387\n",
            "Epoch 439/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0305 - accuracy: 0.9979 - val_loss: 0.7504 - val_accuracy: 0.8368\n",
            "Epoch 440/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0299 - accuracy: 0.9986 - val_loss: 0.7534 - val_accuracy: 0.8330\n",
            "Epoch 441/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0312 - accuracy: 0.9960 - val_loss: 0.7421 - val_accuracy: 0.8349\n",
            "Epoch 442/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0293 - accuracy: 0.9972 - val_loss: 0.7480 - val_accuracy: 0.8349\n",
            "Epoch 443/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0296 - accuracy: 0.9972 - val_loss: 0.7434 - val_accuracy: 0.8387\n",
            "Epoch 444/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0301 - accuracy: 0.9967 - val_loss: 0.7561 - val_accuracy: 0.8349\n",
            "Epoch 445/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0292 - accuracy: 0.9972 - val_loss: 0.7462 - val_accuracy: 0.8311\n",
            "Epoch 446/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0280 - accuracy: 0.9972 - val_loss: 0.7498 - val_accuracy: 0.8387\n",
            "Epoch 447/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0327 - accuracy: 0.9960 - val_loss: 0.7484 - val_accuracy: 0.8406\n",
            "Epoch 448/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0308 - accuracy: 0.9967 - val_loss: 0.7508 - val_accuracy: 0.8387\n",
            "Epoch 449/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0305 - accuracy: 0.9950 - val_loss: 0.7551 - val_accuracy: 0.8349\n",
            "Epoch 450/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0287 - accuracy: 0.9967 - val_loss: 0.7524 - val_accuracy: 0.8368\n",
            "Epoch 451/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0302 - accuracy: 0.9953 - val_loss: 0.7485 - val_accuracy: 0.8349\n",
            "Epoch 452/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0283 - accuracy: 0.9976 - val_loss: 0.7514 - val_accuracy: 0.8311\n",
            "Epoch 453/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0299 - accuracy: 0.9969 - val_loss: 0.7543 - val_accuracy: 0.8330\n",
            "Epoch 454/2000\n",
            "132/132 [==============================] - 1s 8ms/step - loss: 0.0254 - accuracy: 0.9976 - val_loss: 0.7547 - val_accuracy: 0.8349\n",
            "Epoch 455/2000\n",
            "132/132 [==============================] - 1s 7ms/step - loss: 0.0294 - accuracy: 0.9964 - val_loss: 0.7528 - val_accuracy: 0.8368\n",
            "Epoch 456/2000\n",
            "132/132 [==============================] - 1s 7ms/step - loss: 0.0302 - accuracy: 0.9962 - val_loss: 0.7506 - val_accuracy: 0.8387\n",
            "Epoch 457/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0248 - accuracy: 0.9983 - val_loss: 0.7544 - val_accuracy: 0.8368\n",
            "Epoch 458/2000\n",
            "132/132 [==============================] - 1s 8ms/step - loss: 0.0258 - accuracy: 0.9969 - val_loss: 0.7561 - val_accuracy: 0.8349\n",
            "Epoch 459/2000\n",
            "132/132 [==============================] - 1s 10ms/step - loss: 0.0277 - accuracy: 0.9974 - val_loss: 0.7568 - val_accuracy: 0.8330\n",
            "Epoch 460/2000\n",
            "132/132 [==============================] - 1s 9ms/step - loss: 0.0283 - accuracy: 0.9960 - val_loss: 0.7514 - val_accuracy: 0.8273\n",
            "Epoch 461/2000\n",
            "132/132 [==============================] - 1s 9ms/step - loss: 0.0284 - accuracy: 0.9957 - val_loss: 0.7560 - val_accuracy: 0.8387\n",
            "Epoch 462/2000\n",
            "132/132 [==============================] - 1s 8ms/step - loss: 0.0295 - accuracy: 0.9967 - val_loss: 0.7549 - val_accuracy: 0.8387\n",
            "Epoch 463/2000\n",
            "132/132 [==============================] - 1s 7ms/step - loss: 0.0274 - accuracy: 0.9967 - val_loss: 0.7523 - val_accuracy: 0.8387\n",
            "Epoch 464/2000\n",
            "132/132 [==============================] - 1s 8ms/step - loss: 0.0262 - accuracy: 0.9976 - val_loss: 0.7446 - val_accuracy: 0.8349\n",
            "Epoch 465/2000\n",
            "132/132 [==============================] - 1s 7ms/step - loss: 0.0257 - accuracy: 0.9974 - val_loss: 0.7575 - val_accuracy: 0.8368\n",
            "Epoch 466/2000\n",
            "132/132 [==============================] - 1s 9ms/step - loss: 0.0252 - accuracy: 0.9988 - val_loss: 0.7582 - val_accuracy: 0.8349\n",
            "Epoch 467/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0278 - accuracy: 0.9969 - val_loss: 0.7534 - val_accuracy: 0.8406\n",
            "Epoch 468/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0262 - accuracy: 0.9964 - val_loss: 0.7478 - val_accuracy: 0.8387\n",
            "Epoch 469/2000\n",
            "132/132 [==============================] - 1s 9ms/step - loss: 0.0223 - accuracy: 0.9983 - val_loss: 0.7601 - val_accuracy: 0.8349\n",
            "Epoch 470/2000\n",
            "132/132 [==============================] - 1s 9ms/step - loss: 0.0283 - accuracy: 0.9964 - val_loss: 0.7533 - val_accuracy: 0.8387\n",
            "Epoch 471/2000\n",
            "132/132 [==============================] - 1s 10ms/step - loss: 0.0282 - accuracy: 0.9972 - val_loss: 0.7592 - val_accuracy: 0.8368\n",
            "Epoch 472/2000\n",
            "132/132 [==============================] - 1s 10ms/step - loss: 0.0252 - accuracy: 0.9964 - val_loss: 0.7614 - val_accuracy: 0.8349\n",
            "Epoch 473/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0251 - accuracy: 0.9969 - val_loss: 0.7554 - val_accuracy: 0.8368\n",
            "Epoch 474/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0261 - accuracy: 0.9969 - val_loss: 0.7565 - val_accuracy: 0.8368\n",
            "Epoch 475/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0253 - accuracy: 0.9979 - val_loss: 0.7627 - val_accuracy: 0.8349\n",
            "Epoch 476/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0250 - accuracy: 0.9981 - val_loss: 0.7576 - val_accuracy: 0.8349\n",
            "Epoch 477/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0233 - accuracy: 0.9988 - val_loss: 0.7586 - val_accuracy: 0.8349\n",
            "Epoch 478/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0244 - accuracy: 0.9983 - val_loss: 0.7542 - val_accuracy: 0.8387\n",
            "Epoch 479/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0271 - accuracy: 0.9957 - val_loss: 0.7563 - val_accuracy: 0.8425\n",
            "Epoch 480/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0257 - accuracy: 0.9967 - val_loss: 0.7690 - val_accuracy: 0.8330\n",
            "Epoch 481/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0216 - accuracy: 0.9983 - val_loss: 0.7626 - val_accuracy: 0.8368\n",
            "Epoch 482/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0261 - accuracy: 0.9957 - val_loss: 0.7643 - val_accuracy: 0.8349\n",
            "Epoch 483/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0250 - accuracy: 0.9967 - val_loss: 0.7593 - val_accuracy: 0.8349\n",
            "Epoch 484/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0249 - accuracy: 0.9976 - val_loss: 0.7662 - val_accuracy: 0.8311\n",
            "Epoch 485/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0225 - accuracy: 0.9972 - val_loss: 0.7558 - val_accuracy: 0.8387\n",
            "Epoch 486/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0240 - accuracy: 0.9974 - val_loss: 0.7774 - val_accuracy: 0.8292\n",
            "Epoch 487/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0252 - accuracy: 0.9969 - val_loss: 0.7652 - val_accuracy: 0.8387\n",
            "Epoch 488/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0206 - accuracy: 0.9981 - val_loss: 0.7652 - val_accuracy: 0.8368\n",
            "Epoch 489/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0236 - accuracy: 0.9967 - val_loss: 0.7637 - val_accuracy: 0.8406\n",
            "Epoch 490/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0242 - accuracy: 0.9974 - val_loss: 0.7653 - val_accuracy: 0.8311\n",
            "Epoch 491/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0212 - accuracy: 0.9974 - val_loss: 0.7745 - val_accuracy: 0.8368\n",
            "Epoch 492/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0237 - accuracy: 0.9969 - val_loss: 0.7631 - val_accuracy: 0.8368\n",
            "Epoch 493/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0230 - accuracy: 0.9972 - val_loss: 0.7602 - val_accuracy: 0.8387\n",
            "Epoch 494/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0241 - accuracy: 0.9972 - val_loss: 0.7648 - val_accuracy: 0.8349\n",
            "Epoch 495/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0224 - accuracy: 0.9972 - val_loss: 0.7578 - val_accuracy: 0.8330\n",
            "Epoch 496/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0216 - accuracy: 0.9983 - val_loss: 0.7656 - val_accuracy: 0.8368\n",
            "Epoch 497/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0219 - accuracy: 0.9983 - val_loss: 0.7662 - val_accuracy: 0.8387\n",
            "Epoch 498/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0237 - accuracy: 0.9964 - val_loss: 0.7829 - val_accuracy: 0.8330\n",
            "Epoch 499/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0207 - accuracy: 0.9976 - val_loss: 0.7719 - val_accuracy: 0.8311\n",
            "Epoch 500/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0226 - accuracy: 0.9974 - val_loss: 0.7657 - val_accuracy: 0.8311\n",
            "Epoch 501/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0213 - accuracy: 0.9981 - val_loss: 0.7758 - val_accuracy: 0.8330\n",
            "Epoch 502/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0238 - accuracy: 0.9974 - val_loss: 0.7636 - val_accuracy: 0.8349\n",
            "Epoch 503/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0226 - accuracy: 0.9972 - val_loss: 0.7673 - val_accuracy: 0.8387\n",
            "Epoch 504/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0214 - accuracy: 0.9979 - val_loss: 0.7751 - val_accuracy: 0.8387\n",
            "Epoch 505/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0209 - accuracy: 0.9972 - val_loss: 0.7730 - val_accuracy: 0.8349\n",
            "Epoch 506/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0252 - accuracy: 0.9964 - val_loss: 0.7698 - val_accuracy: 0.8330\n",
            "Epoch 507/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0241 - accuracy: 0.9964 - val_loss: 0.7657 - val_accuracy: 0.8349\n",
            "Epoch 508/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0216 - accuracy: 0.9969 - val_loss: 0.7693 - val_accuracy: 0.8368\n",
            "Epoch 509/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0223 - accuracy: 0.9969 - val_loss: 0.7713 - val_accuracy: 0.8387\n",
            "Epoch 510/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.9981 - val_loss: 0.7734 - val_accuracy: 0.8368\n",
            "Epoch 511/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0226 - accuracy: 0.9974 - val_loss: 0.7750 - val_accuracy: 0.8368\n",
            "Epoch 512/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0214 - accuracy: 0.9964 - val_loss: 0.7724 - val_accuracy: 0.8311\n",
            "Epoch 513/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0209 - accuracy: 0.9976 - val_loss: 0.7756 - val_accuracy: 0.8349\n",
            "Epoch 514/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.9983 - val_loss: 0.7706 - val_accuracy: 0.8330\n",
            "Epoch 515/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0218 - accuracy: 0.9974 - val_loss: 0.7782 - val_accuracy: 0.8330\n",
            "Epoch 516/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0193 - accuracy: 0.9988 - val_loss: 0.7834 - val_accuracy: 0.8292\n",
            "Epoch 517/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0211 - accuracy: 0.9979 - val_loss: 0.7754 - val_accuracy: 0.8349\n",
            "Epoch 518/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0202 - accuracy: 0.9976 - val_loss: 0.7750 - val_accuracy: 0.8406\n",
            "Epoch 519/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0188 - accuracy: 0.9976 - val_loss: 0.7831 - val_accuracy: 0.8330\n",
            "Epoch 520/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0201 - accuracy: 0.9974 - val_loss: 0.7809 - val_accuracy: 0.8368\n",
            "Epoch 521/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0191 - accuracy: 0.9981 - val_loss: 0.7779 - val_accuracy: 0.8330\n",
            "Epoch 522/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0213 - accuracy: 0.9974 - val_loss: 0.7892 - val_accuracy: 0.8387\n",
            "Epoch 523/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0183 - accuracy: 0.9986 - val_loss: 0.7834 - val_accuracy: 0.8368\n",
            "Epoch 524/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0214 - accuracy: 0.9986 - val_loss: 0.7754 - val_accuracy: 0.8368\n",
            "Epoch 525/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0178 - accuracy: 0.9983 - val_loss: 0.7847 - val_accuracy: 0.8292\n",
            "Epoch 526/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0248 - accuracy: 0.9955 - val_loss: 0.7850 - val_accuracy: 0.8292\n",
            "Epoch 527/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0188 - accuracy: 0.9972 - val_loss: 0.7824 - val_accuracy: 0.8330\n",
            "Epoch 528/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0192 - accuracy: 0.9979 - val_loss: 0.7759 - val_accuracy: 0.8330\n",
            "Epoch 529/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0179 - accuracy: 0.9981 - val_loss: 0.7833 - val_accuracy: 0.8330\n",
            "Epoch 530/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0188 - accuracy: 0.9981 - val_loss: 0.7722 - val_accuracy: 0.8387\n",
            "Epoch 531/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0184 - accuracy: 0.9976 - val_loss: 0.7862 - val_accuracy: 0.8311\n",
            "Epoch 532/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0197 - accuracy: 0.9972 - val_loss: 0.7882 - val_accuracy: 0.8292\n",
            "Epoch 533/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0201 - accuracy: 0.9969 - val_loss: 0.7776 - val_accuracy: 0.8368\n",
            "Epoch 534/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0170 - accuracy: 0.9983 - val_loss: 0.7807 - val_accuracy: 0.8330\n",
            "Epoch 535/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0198 - accuracy: 0.9979 - val_loss: 0.7812 - val_accuracy: 0.8387\n",
            "Epoch 536/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0191 - accuracy: 0.9976 - val_loss: 0.7758 - val_accuracy: 0.8330\n",
            "Epoch 537/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0195 - accuracy: 0.9974 - val_loss: 0.7802 - val_accuracy: 0.8349\n",
            "Epoch 538/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0198 - accuracy: 0.9974 - val_loss: 0.7859 - val_accuracy: 0.8292\n",
            "Epoch 539/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0176 - accuracy: 0.9986 - val_loss: 0.7970 - val_accuracy: 0.8254\n",
            "Epoch 540/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0190 - accuracy: 0.9974 - val_loss: 0.7915 - val_accuracy: 0.8311\n",
            "Epoch 541/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0204 - accuracy: 0.9969 - val_loss: 0.8041 - val_accuracy: 0.8292\n",
            "Epoch 542/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0189 - accuracy: 0.9972 - val_loss: 0.7752 - val_accuracy: 0.8368\n",
            "Epoch 543/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0199 - accuracy: 0.9979 - val_loss: 0.7815 - val_accuracy: 0.8330\n",
            "Epoch 544/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0173 - accuracy: 0.9983 - val_loss: 0.7835 - val_accuracy: 0.8330\n",
            "Epoch 545/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0177 - accuracy: 0.9981 - val_loss: 0.7785 - val_accuracy: 0.8330\n",
            "Epoch 546/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0184 - accuracy: 0.9972 - val_loss: 0.7873 - val_accuracy: 0.8349\n",
            "Epoch 547/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0198 - accuracy: 0.9974 - val_loss: 0.7854 - val_accuracy: 0.8349\n",
            "Epoch 548/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0188 - accuracy: 0.9972 - val_loss: 0.7858 - val_accuracy: 0.8368\n",
            "Epoch 549/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0172 - accuracy: 0.9983 - val_loss: 0.7844 - val_accuracy: 0.8330\n",
            "Epoch 550/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0175 - accuracy: 0.9981 - val_loss: 0.7824 - val_accuracy: 0.8330\n",
            "Epoch 551/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0193 - accuracy: 0.9974 - val_loss: 0.7976 - val_accuracy: 0.8311\n",
            "Epoch 552/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0172 - accuracy: 0.9988 - val_loss: 0.7841 - val_accuracy: 0.8311\n",
            "Epoch 553/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0178 - accuracy: 0.9976 - val_loss: 0.7865 - val_accuracy: 0.8292\n",
            "Epoch 554/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0175 - accuracy: 0.9983 - val_loss: 0.7820 - val_accuracy: 0.8349\n",
            "Epoch 555/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0204 - accuracy: 0.9964 - val_loss: 0.7921 - val_accuracy: 0.8311\n",
            "Epoch 556/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0172 - accuracy: 0.9979 - val_loss: 0.7973 - val_accuracy: 0.8292\n",
            "Epoch 557/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0168 - accuracy: 0.9981 - val_loss: 0.7953 - val_accuracy: 0.8349\n",
            "Epoch 558/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0165 - accuracy: 0.9981 - val_loss: 0.7894 - val_accuracy: 0.8349\n",
            "Epoch 559/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0163 - accuracy: 0.9981 - val_loss: 0.7971 - val_accuracy: 0.8311\n",
            "Epoch 560/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0180 - accuracy: 0.9986 - val_loss: 0.7855 - val_accuracy: 0.8406\n",
            "Epoch 561/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0187 - accuracy: 0.9976 - val_loss: 0.7869 - val_accuracy: 0.8254\n",
            "Epoch 562/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0180 - accuracy: 0.9974 - val_loss: 0.7915 - val_accuracy: 0.8368\n",
            "Epoch 563/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0173 - accuracy: 0.9981 - val_loss: 0.7908 - val_accuracy: 0.8368\n",
            "Epoch 564/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0204 - accuracy: 0.9964 - val_loss: 0.7833 - val_accuracy: 0.8330\n",
            "Epoch 565/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0163 - accuracy: 0.9976 - val_loss: 0.7863 - val_accuracy: 0.8387\n",
            "Epoch 566/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0169 - accuracy: 0.9972 - val_loss: 0.7949 - val_accuracy: 0.8349\n",
            "Epoch 567/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0158 - accuracy: 0.9983 - val_loss: 0.8041 - val_accuracy: 0.8311\n",
            "Epoch 568/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0200 - accuracy: 0.9967 - val_loss: 0.8027 - val_accuracy: 0.8349\n",
            "Epoch 569/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0171 - accuracy: 0.9979 - val_loss: 0.7942 - val_accuracy: 0.8330\n",
            "Epoch 570/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0174 - accuracy: 0.9983 - val_loss: 0.7953 - val_accuracy: 0.8349\n",
            "Epoch 571/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0210 - accuracy: 0.9967 - val_loss: 0.7932 - val_accuracy: 0.8273\n",
            "Epoch 572/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0164 - accuracy: 0.9981 - val_loss: 0.7892 - val_accuracy: 0.8330\n",
            "Epoch 573/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0167 - accuracy: 0.9979 - val_loss: 0.7931 - val_accuracy: 0.8273\n",
            "Epoch 574/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0164 - accuracy: 0.9979 - val_loss: 0.7977 - val_accuracy: 0.8330\n",
            "Epoch 575/2000\n",
            "132/132 [==============================] - 1s 7ms/step - loss: 0.0157 - accuracy: 0.9991 - val_loss: 0.7962 - val_accuracy: 0.8330\n",
            "Epoch 576/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0159 - accuracy: 0.9986 - val_loss: 0.7910 - val_accuracy: 0.8349\n",
            "Epoch 577/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0159 - accuracy: 0.9979 - val_loss: 0.8009 - val_accuracy: 0.8330\n",
            "Epoch 578/2000\n",
            "132/132 [==============================] - 1s 7ms/step - loss: 0.0171 - accuracy: 0.9983 - val_loss: 0.7940 - val_accuracy: 0.8349\n",
            "Epoch 579/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0157 - accuracy: 0.9979 - val_loss: 0.7918 - val_accuracy: 0.8349\n",
            "Epoch 580/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0154 - accuracy: 0.9983 - val_loss: 0.7971 - val_accuracy: 0.8273\n",
            "Epoch 581/2000\n",
            "132/132 [==============================] - 1s 6ms/step - loss: 0.0146 - accuracy: 0.9986 - val_loss: 0.7898 - val_accuracy: 0.8292\n",
            "Epoch 582/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0161 - accuracy: 0.9981 - val_loss: 0.7979 - val_accuracy: 0.8349\n",
            "Epoch 583/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0151 - accuracy: 0.9983 - val_loss: 0.7921 - val_accuracy: 0.8311\n",
            "Epoch 584/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0172 - accuracy: 0.9974 - val_loss: 0.7924 - val_accuracy: 0.8311\n",
            "Epoch 585/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0171 - accuracy: 0.9979 - val_loss: 0.7930 - val_accuracy: 0.8311\n",
            "Epoch 586/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0160 - accuracy: 0.9983 - val_loss: 0.7874 - val_accuracy: 0.8368\n",
            "Epoch 587/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0159 - accuracy: 0.9983 - val_loss: 0.8021 - val_accuracy: 0.8216\n",
            "Epoch 588/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0152 - accuracy: 0.9974 - val_loss: 0.7954 - val_accuracy: 0.8387\n",
            "Epoch 589/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0136 - accuracy: 0.9986 - val_loss: 0.7970 - val_accuracy: 0.8330\n",
            "Epoch 590/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0132 - accuracy: 0.9988 - val_loss: 0.7980 - val_accuracy: 0.8349\n",
            "Epoch 591/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0153 - accuracy: 0.9993 - val_loss: 0.7990 - val_accuracy: 0.8330\n",
            "Epoch 592/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0149 - accuracy: 0.9983 - val_loss: 0.7941 - val_accuracy: 0.8368\n",
            "Epoch 593/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0139 - accuracy: 0.9983 - val_loss: 0.7973 - val_accuracy: 0.8330\n",
            "Epoch 594/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0168 - accuracy: 0.9991 - val_loss: 0.7891 - val_accuracy: 0.8368\n",
            "Epoch 595/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0154 - accuracy: 0.9983 - val_loss: 0.8043 - val_accuracy: 0.8311\n",
            "Epoch 596/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0132 - accuracy: 0.9995 - val_loss: 0.8130 - val_accuracy: 0.8311\n",
            "Epoch 597/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0145 - accuracy: 0.9991 - val_loss: 0.8033 - val_accuracy: 0.8311\n",
            "Epoch 598/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0147 - accuracy: 0.9983 - val_loss: 0.8088 - val_accuracy: 0.8273\n",
            "Epoch 599/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0139 - accuracy: 0.9988 - val_loss: 0.7971 - val_accuracy: 0.8368\n",
            "Epoch 600/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0150 - accuracy: 0.9986 - val_loss: 0.8015 - val_accuracy: 0.8273\n",
            "Epoch 601/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0166 - accuracy: 0.9981 - val_loss: 0.8116 - val_accuracy: 0.8368\n",
            "Epoch 602/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0162 - accuracy: 0.9974 - val_loss: 0.8144 - val_accuracy: 0.8330\n",
            "Epoch 603/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0151 - accuracy: 0.9967 - val_loss: 0.7952 - val_accuracy: 0.8330\n",
            "Epoch 604/2000\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.0144 - accuracy: 0.9988 - val_loss: 0.8037 - val_accuracy: 0.8311\n",
            "Epoch 605/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0137 - accuracy: 0.9988 - val_loss: 0.7890 - val_accuracy: 0.8387\n",
            "Epoch 606/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0148 - accuracy: 0.9974 - val_loss: 0.8193 - val_accuracy: 0.8273\n",
            "Epoch 607/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0150 - accuracy: 0.9983 - val_loss: 0.7957 - val_accuracy: 0.8349\n",
            "Epoch 608/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0153 - accuracy: 0.9979 - val_loss: 0.8048 - val_accuracy: 0.8330\n",
            "Epoch 609/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0153 - accuracy: 0.9981 - val_loss: 0.8065 - val_accuracy: 0.8292\n",
            "Epoch 610/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0151 - accuracy: 0.9981 - val_loss: 0.8000 - val_accuracy: 0.8311\n",
            "Epoch 611/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0147 - accuracy: 0.9983 - val_loss: 0.8067 - val_accuracy: 0.8330\n",
            "Epoch 612/2000\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.0153 - accuracy: 0.9976 - val_loss: 0.8054 - val_accuracy: 0.8311\n",
            "Epoch 612: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing loss and accuracy of the model on the test set\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FksY9EwwHDkd",
        "outputId": "6b5ad10d-8c28-495b-a8e8-d7741f9957c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 0s 2ms/step - loss: 0.8913 - accuracy: 0.8444\n",
            "Loss:  0.8912551999092102\n",
            "Accuracy:  0.8444022536277771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the history of training and its keys\n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPdyDruxHIv1",
        "outputId": "b413b963-199e-4901-8146-6dfc91cef459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)"
      ],
      "metadata": {
        "id": "shbF_xRlHDqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting of loss\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "iLsoTgHGP7Oq",
        "outputId": "3591eede-8400-4dd0-8bc7-7f558fc176af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting of accuracy\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "1uWqq-CtP-Lq",
        "outputId": "56fee659-2369-4c78-c4ad-d69ab9275908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the probability model for testing\n",
        "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
        "\n",
        "# predicting test samples\n",
        "predictions = probability_model.predict(raw_test_batch.map(vectorize_text))"
      ],
      "metadata": {
        "id": "YQj61lIMvX-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "519c03c0-33ad-4962-d89d-36641c995966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# showing the first test sample result label\n",
        "np.argmax(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cbhBKR7NYPQ",
        "outputId": "bcf37194-a099-47bf-a735-8b46ec324977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# showing the true label of the first test sample\n",
        "test_df.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNsnTWVuu3xj",
        "outputId": "b5893ac7-fc9e-4fdb-cf2d-bb00e934535f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text         kazoo tough giraffe dog toy kazoo brand kazoo ...\n",
              "label                                   Animals & Pet Supplies\n",
              "label_int                                                    0\n",
              "Name: 8, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bOt2UPu7NbUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Author:** https://farrokhkarimi.github.io/"
      ],
      "metadata": {
        "id": "CSw7kwIQipyc"
      }
    }
  ]
}